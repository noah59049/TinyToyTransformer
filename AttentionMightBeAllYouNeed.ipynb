{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1cf4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089b062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, add_stop_token = True):\n",
    "    tokens = []\n",
    "    for letter in text:\n",
    "        if letter in TOKEN_DICT:\n",
    "            tokens.append(TOKEN_DICT[letter])\n",
    "        else:\n",
    "            # UNK token\n",
    "            tokens.append(len(TOKEN_DICT))\n",
    "            \n",
    "    # STOP token\n",
    "    if add_stop_token:\n",
    "        tokens.append(len(TOKEN_DICT) + 1)\n",
    "    return torch.tensor(tokens)\n",
    "\n",
    "def read_tokens_from_file(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        text = file.read()\n",
    "        return tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be0ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_giga_token_counts(verbose = False):\n",
    "    mega_text = \"\"\n",
    "    for directory in [\"AA\", \"AB\", \"AC\"]:\n",
    "        for digit1 in range(10):\n",
    "            for digit2 in range(10):\n",
    "                filename = f\"text/{directory}/wiki_{digit1}{digit2}\"\n",
    "                try:\n",
    "                    with open(filename, \"r\") as file:\n",
    "                        text = file.read()\n",
    "                        mega_text += text\n",
    "                except FileNotFoundError:\n",
    "                    if verbose:\n",
    "                        print(f\"File {filename} not found\")\n",
    "                    \n",
    "    return mega_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c295972",
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_text = get_giga_token_counts()\n",
    "token_counts = pd.Series(list(mega_text)).value_counts()\n",
    "token_counts = token_counts[token_counts > 300]\n",
    "TOKEN_DICT = {e:i for i, e in enumerate(token_counts.index)}\n",
    "REVERSE_TOKEN_DICT = {i:e for i, e in enumerate(token_counts.index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d1ba341",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 96\n",
    "DK = 32\n",
    "N_HEADS = 16\n",
    "CONTEXT_SIZE = 24\n",
    "MLP_HIDDEN_LAYER_SIZE = 192\n",
    "N_DISTINCT_TOKENS = len(TOKEN_DICT) + 2 # The + 2 is for UNK and STOP tokens\n",
    "N_LAYERS = 20\n",
    "P_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3258665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_matrix(dim):\n",
    "    neginf = (torch.tensor(-1) / torch.tensor(0)).item()\n",
    "    matrix = torch.zeros(dim, dim)\n",
    "    for i in range(dim):\n",
    "        for j in range(i + 1, dim):\n",
    "            matrix[i][j] = neginf\n",
    "            \n",
    "    return matrix\n",
    "MASK_MATRIX = create_mask_matrix(CONTEXT_SIZE).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133f1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embeddings(d_model, context_size):\n",
    "    a = torch.zeros(context_size, d_model)\n",
    "    for pos in range(context_size):\n",
    "        for j in range(d_model):\n",
    "            if j % 2 == 0:\n",
    "                a[pos][j] = math.sin(pos / 10000 ** (j / d_model))\n",
    "            else:\n",
    "                a[pos][j] = math.cos(pos / 10000 ** ((j - 1) / d_model))\n",
    "    return a\n",
    "                \n",
    "def positional_embeddings2(d_model, max_len):\n",
    "    # Source: https://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(\n",
    "        torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "    )\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "# These are the exact same way of getting positional embeddings, barring floating-point precision issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4462957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.Wk = nn.Linear(D_MODEL, DK, bias = False)\n",
    "        self.Wq = nn.Linear(D_MODEL, DK, bias = False)\n",
    "        self.Wv = nn.Linear(D_MODEL, DK, bias = False)\n",
    "        self.dropout = nn.Dropout(p = P_DROPOUT)\n",
    "        \n",
    "        # The weights are initialized with Kaiming uniform by default, which is fine for now\n",
    "        \n",
    "    def forward(self, x):\n",
    "        K = self.Wk(x)\n",
    "        Q = self.Wq(x)\n",
    "        V = self.Wv(x)\n",
    "        \n",
    "        QKT = torch.matmul(Q, K.transpose(dim0 = 1, dim1 = 2))\n",
    "        QKT += MASK_MATRIX\n",
    "        pattern = torch.softmax(QKT / math.sqrt(DK), dim = 2)\n",
    "        pattern = self.dropout(pattern)\n",
    "        return torch.matmul(pattern, V)\n",
    "    \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead() for i in range(N_HEADS)])\n",
    "        self.Wo = nn.Linear(DK * N_HEADS, D_MODEL, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.Wo(\n",
    "                torch.concat(\n",
    "                    [head(x) for head in self.heads],\n",
    "                        dim = 2))\n",
    "    \n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.layer1 = nn.Linear(D_MODEL, MLP_HIDDEN_LAYER_SIZE)\n",
    "        self.activation1 = nn.GELU()\n",
    "        self.layer2 = nn.Linear(MLP_HIDDEN_LAYER_SIZE, D_MODEL)\n",
    "        # The nn.Linear layers are initialized with Kaiming uniform initialization by default, which is fine\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.activation1(x1)\n",
    "        x3 = self.layer2(x2)\n",
    "        return x3\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Embedding, positional encoding, and dropout\n",
    "        self.embedding = nn.Embedding(N_DISTINCT_TOKENS, D_MODEL)\n",
    "        self.positional_embedding = positional_embeddings(D_MODEL, CONTEXT_SIZE).unsqueeze(0)\n",
    "        self.first_dropout = nn.Dropout(p = P_DROPOUT)\n",
    "\n",
    "        # Attention\n",
    "        self.attention_blocks = nn.ModuleList()\n",
    "        for i in range(N_LAYERS):\n",
    "            layer_norm = nn.LayerNorm(D_MODEL)\n",
    "            attention = AttentionBlock()\n",
    "            dropout = nn.Dropout(P_DROPOUT)\n",
    "            module = nn.Sequential(layer_norm, attention, dropout)\n",
    "            self.attention_blocks.append(module)\n",
    "            \n",
    "        # MLP\n",
    "        self.mlps = nn.ModuleList()\n",
    "        for i in range(N_LAYERS):\n",
    "            layer_norm = nn.LayerNorm(D_MODEL)\n",
    "            mlp = MLPBlock()\n",
    "            dropout = nn.Dropout(P_DROPOUT)\n",
    "            module = nn.Sequential(layer_norm, mlp, dropout)\n",
    "            self.mlps.append(module)\n",
    "            \n",
    "        # Final layer norm and linear\n",
    "        self.final_layer_norm = nn.LayerNorm(D_MODEL)\n",
    "        self.final_linear = nn.Linear(D_MODEL, N_DISTINCT_TOKENS)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Step 1: Embedding and positional encoding\n",
    "        x = self.embedding(x) + self.positional_embedding\n",
    "        \n",
    "        # Step 2: First dropout\n",
    "        x = self.first_dropout(x)\n",
    "        \n",
    "        # Step 3: All the blocks\n",
    "        for i in range(N_LAYERS):\n",
    "            x = x + self.attention_blocks[i](x)\n",
    "            x = x + self.mlps[i](x)\n",
    "            \n",
    "        # Step 4: Final layer norm\n",
    "        x = self.final_layer_norm(x)\n",
    "        \n",
    "        # Step 5: Final linear layer to get predictions\n",
    "        x = self.final_linear(x)\n",
    "        \n",
    "        # Step 6: Return\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6d592fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_peeking_test():\n",
    "    my_model = Transformer()\n",
    "    my_model.eval()\n",
    "    for pos in range(CONTEXT_SIZE):\n",
    "        x = torch.arange(0, CONTEXT_SIZE * 2, 2).unsqueeze(0)\n",
    "        output1 = my_model(x)\n",
    "        for disturb_pos in range(pos + 1, CONTEXT_SIZE):\n",
    "            x[0][disturb_pos] += 1\n",
    "            output2 = my_model(x)\n",
    "            (output1[0,:pos + 1] == output2[0,:pos + 1]).all()\n",
    "            \n",
    "no_peeking_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa7b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonzero_gradient_test():\n",
    "    my_model = Transformer()\n",
    "    x = torch.arange(0, 26, 2)\n",
    "    out = my_model(x)\n",
    "    loss = out.mean()\n",
    "    loss.backward()\n",
    "    print([e.grad for e in my_model.parameters()])\n",
    "    \n",
    "# nonzero_gradient_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5a444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonzero_gradient_test():\n",
    "    my_model = Transformer()\n",
    "\n",
    "    # 1) turn off dropout\n",
    "    my_model.eval()\n",
    "\n",
    "    # 2) (optional) fix RNG\n",
    "    torch.manual_seed(1234)\n",
    "    torch.cuda.manual_seed_all(1234)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    # 3) run forward & backward\n",
    "    x = torch.arange(0, 26, 2)           # your token indices\n",
    "    out = my_model(x)\n",
    "    loss = out.mean()\n",
    "    loss.backward()\n",
    "\n",
    "    # 4) inspect gradients\n",
    "    for name, param in my_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            g = param.grad\n",
    "            print(f\"{name:30s} → norm = {g.norm().item():.3e}\")\n",
    "        else:\n",
    "            g = param.grad\n",
    "            print(f\"DOES NOT REQUIRE GRAD: {name:30s} → norm = {g.norm().item():.3e}\")\n",
    "    \n",
    "# nonzero_gradient_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "984f8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokens):\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for i in range(0, len(tokens) - CONTEXT_SIZE - 1, CONTEXT_SIZE):\n",
    "        X = tokens[i     : i + CONTEXT_SIZE    ]\n",
    "        y = tokens[i + 1 : i + CONTEXT_SIZE + 1]\n",
    "        Xs.append(X)\n",
    "        ys.append(y)\n",
    "\n",
    "    X = torch.stack(Xs)\n",
    "    y = torch.stack(ys)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb8ff33e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loss_fn, tokens, batch_size = 32):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    X_all, y_all = get_dataset(tokens)\n",
    "    loss_vals = []\n",
    "    for i in range(0, X_all.shape[0], batch_size):\n",
    "        X = X_all[i : i + batch_size]\n",
    "        y = y_all[i : i + batch_size]\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1, N_DISTINCT_TOKENS), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i == 0:\n",
    "            for param in model.parameters():\n",
    "                print(param.grad)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_vals.append(loss.item())\n",
    "        t1 = time.time()\n",
    "        \n",
    "    batch_loss = torch.mean(torch.tensor(loss_vals))\n",
    "    print(f\"{i + 1} samples evaluated, time={int(t1-t0)}, loss = {batch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88df17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch_on_file(model, optimizer, loss_fn, filename, batch_size = 32):\n",
    "    train_one_epoch(model, optimizer, loss_fn, read_tokens_from_file(filename), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb3b2e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40961 samples evaluated, time=1143, loss = 3.7329\n",
      "40961 samples evaluated, time=1043, loss = 2.9423\n",
      "40961 samples evaluated, time=970, loss = 2.7601\n",
      "43009 samples evaluated, time=1074, loss = 2.6713\n",
      "43009 samples evaluated, time=974, loss = 2.6222\n",
      "43009 samples evaluated, time=1038, loss = 2.5703\n",
      "43009 samples evaluated, time=1028, loss = 2.4927\n",
      "40961 samples evaluated, time=1062, loss = 2.4342\n",
      "40961 samples evaluated, time=1163, loss = 2.3851\n",
      "43009 samples evaluated, time=1052, loss = 2.3714\n"
     ]
    }
   ],
   "source": [
    "my_model = Transformer()\n",
    "for digit1 in range(1):\n",
    "    for digit2 in range(10):\n",
    "        run_epoch_on_file(my_model,\n",
    "                        optimizer = torch.optim.Adam(my_model.parameters()),\n",
    "                        loss_fn = nn.CrossEntropyLoss(),\n",
    "                        filename = f\"text/AA/wiki_{digit1}{digit2}\",\n",
    "                        batch_size = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c54259e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, text):\n",
    "    model.eval()\n",
    "    trimmed = text[-CONTEXT_SIZE:]\n",
    "    tokens = tokenize(trimmed, add_stop_token = False).unsqueeze(0)\n",
    "    output = model(tokens)\n",
    "    logits = output[:,0:1,:].squeeze()\n",
    "    probs = torch.softmax(logits, dim = 0) # We can do argmax just on the logits if we want, but whatever.\n",
    "    token_id = torch.argmax(logits).item()\n",
    "    return REVERSE_TOKEN_DICT[token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "658873e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_tokens(model, text, num_tokens):\n",
    "    model.eval()\n",
    "    for i in range(num_tokens):\n",
    "        text += predict_next_token(model, text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06174bc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anarchism is a political  neten etn tntrnenhntnett  h  t h th he   e h  hhttetthtethete ttt tetteehh hheh he h thhhth hh  eet'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_tokens(my_model, \"Anarchism is a political \", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50258feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_error(model, loss_fn, tokens, batch_size = 1024):\n",
    "    model.eval()\n",
    "    t0 = time.time()\n",
    "    X_all, y_all = get_dataset(tokens)\n",
    "    loss_vals = []\n",
    "    for i in range(0, X_all.shape[0], batch_size):\n",
    "        X = X_all[i : i + batch_size]\n",
    "        y = y_all[i : i + batch_size]\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1, N_DISTINCT_TOKENS), y.view(-1))\n",
    "        loss_vals.append(loss.item())\n",
    "        t1 = time.time()\n",
    "        \n",
    "    batch_loss = torch.mean(torch.tensor(loss_vals))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "404a5a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2822)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_test_error(my_model,\n",
    "               nn.CrossEntropyLoss(),\n",
    "               read_tokens_from_file(\"text/AD/wiki_62\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edda8d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.6801,  0.2517,  0.9339,  ..., -1.1119,  0.2616,  0.5658],\n",
      "        [ 0.7903, -1.4222, -0.4583,  ...,  0.2672,  0.5408,  0.3611],\n",
      "        [ 0.2770, -0.0861,  0.0234,  ..., -1.1143, -1.1699, -0.2858],\n",
      "        ...,\n",
      "        [-0.5868, -0.8638,  0.1274,  ..., -0.1934, -0.3445,  0.5701],\n",
      "        [ 1.0681,  1.7862,  1.1129,  ...,  0.2777,  1.4612,  2.3642],\n",
      "        [-0.8963,  0.8775, -0.5612,  ..., -0.7263, -0.4534,  1.4578]],\n",
      "       requires_grad=True)\n",
      "(198, 96)\n",
      "attention_blocks.0.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0191, 1.0447, 1.0169, 1.0196, 1.0019, 1.0294, 1.0523, 1.0361, 1.0392,\n",
      "        1.0240, 1.0349, 1.0598, 1.0583, 1.0416, 1.0585, 1.0172, 1.0637, 1.0585,\n",
      "        1.0595, 1.0721, 1.0241, 1.0661, 0.9983, 1.0762, 1.0386, 1.0618, 1.0699,\n",
      "        1.1365, 1.1005, 1.0398, 1.0659, 1.0465, 1.0287, 0.9954, 1.0592, 1.0561,\n",
      "        1.0288, 1.0164, 1.0080, 0.9988, 1.0484, 1.0153, 1.0205, 1.0140, 1.0465,\n",
      "        1.0078, 1.0156, 1.0239, 1.0240, 1.0241, 1.0107, 1.0017, 1.0210, 1.0009,\n",
      "        1.0247, 1.0271, 1.0409, 0.9886, 1.0164, 0.9982, 1.0335, 1.0316, 1.0597,\n",
      "        1.0085, 1.0373, 1.0141, 0.9949, 1.0157, 1.0419, 1.0009, 1.0555, 1.0070,\n",
      "        1.0017, 1.0313, 1.0212, 1.0122, 0.9927, 1.0367, 1.0218, 1.0076, 1.0567,\n",
      "        0.9924, 1.0270, 0.9965, 1.0386, 0.9956, 1.0496, 1.0254, 1.0422, 1.0101,\n",
      "        0.9765, 1.0144, 1.0159, 1.0603, 1.0115, 0.9930], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.1.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0393, 1.0112, 1.0072, 0.9904, 1.0106, 1.0046, 1.0652, 1.0381, 1.0039,\n",
      "        1.0436, 1.0530, 1.0531, 1.0631, 1.0295, 1.0466, 1.0293, 1.0639, 1.0261,\n",
      "        1.0314, 1.0542, 1.0201, 1.0292, 1.0074, 1.0420, 1.0347, 1.0747, 1.0354,\n",
      "        1.0554, 1.0186, 1.0105, 0.9950, 1.0061, 1.0282, 1.0000, 1.0307, 1.0395,\n",
      "        1.0309, 0.9973, 1.0474, 0.9899, 1.0460, 1.0063, 1.0442, 1.0188, 1.0383,\n",
      "        1.0302, 1.0034, 0.9973, 1.0211, 1.0499, 1.0370, 1.0070, 1.0235, 1.0089,\n",
      "        1.0195, 1.0483, 1.0105, 1.0008, 1.0254, 1.0068, 1.0153, 1.0374, 0.9772,\n",
      "        1.0116, 1.0228, 1.0529, 1.0037, 1.0269, 1.0388, 1.0265, 1.0414, 1.0139,\n",
      "        1.0540, 1.0407, 1.0236, 1.0186, 0.9947, 1.0154, 1.0250, 1.0262, 1.0407,\n",
      "        1.0141, 1.0245, 1.0303, 1.0282, 1.0685, 1.0259, 1.0280, 1.0525, 1.0210,\n",
      "        1.0089, 1.0257, 1.0156, 1.0299, 1.0229, 1.0125], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.2.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0245, 1.0015, 1.0278, 0.9919, 0.9903, 0.9864, 1.0299, 0.9931, 0.9928,\n",
      "        1.0271, 1.0238, 1.0290, 1.0582, 1.0074, 1.0290, 1.0515, 1.0218, 1.0144,\n",
      "        1.0227, 1.0240, 0.9957, 0.9990, 1.0214, 0.9916, 0.9967, 1.0105, 0.9871,\n",
      "        1.0028, 1.0282, 1.0054, 0.9825, 0.9994, 1.0124, 0.9780, 0.9906, 1.0293,\n",
      "        0.9999, 0.9730, 1.0057, 0.9935, 0.9914, 1.0241, 1.0006, 1.0154, 1.0401,\n",
      "        0.9923, 1.0338, 1.0085, 1.0197, 1.0084, 1.0235, 1.0037, 1.0204, 0.9881,\n",
      "        1.0045, 1.0289, 1.0012, 0.9871, 1.0285, 1.0129, 0.9898, 1.0097, 0.9808,\n",
      "        1.0356, 1.0271, 1.0312, 1.0171, 1.0389, 1.0104, 1.0213, 0.9968, 1.0130,\n",
      "        1.0210, 1.0191, 1.0260, 1.0014, 1.0090, 0.9899, 1.0376, 1.0017, 1.0257,\n",
      "        0.9876, 1.0118, 0.9912, 1.0129, 1.0367, 0.9997, 1.0380, 1.0521, 1.0111,\n",
      "        1.0045, 1.0033, 1.0145, 0.9944, 1.0333, 0.9746], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.3.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0489, 1.0057, 0.9953, 0.9861, 0.9874, 0.9980, 1.0225, 0.9916, 0.9789,\n",
      "        0.9923, 1.0290, 1.0111, 1.0218, 0.9951, 1.0049, 1.0207, 1.0014, 1.0103,\n",
      "        1.0329, 0.9998, 1.0222, 0.9956, 1.0174, 0.9792, 0.9853, 1.0044, 1.0309,\n",
      "        0.9783, 1.0130, 0.9842, 0.9494, 0.9851, 1.0140, 0.9852, 0.9881, 1.0288,\n",
      "        0.9848, 0.9594, 1.0127, 0.9844, 1.0193, 1.0195, 0.9997, 1.0066, 1.0263,\n",
      "        0.9649, 0.9946, 0.9725, 1.0200, 1.0380, 1.0177, 0.9947, 1.0050, 0.9859,\n",
      "        1.0103, 1.0266, 0.9950, 0.9925, 0.9994, 1.0098, 0.9861, 1.0549, 0.9811,\n",
      "        1.0075, 1.0089, 1.0036, 1.0028, 1.0281, 1.0116, 1.0129, 0.9890, 0.9921,\n",
      "        1.0045, 1.0084, 1.0056, 0.9969, 0.9939, 0.9722, 1.0413, 0.9946, 1.0409,\n",
      "        1.0184, 1.0376, 0.9813, 0.9882, 1.0327, 0.9989, 1.0282, 1.0408, 0.9883,\n",
      "        1.0124, 0.9995, 1.0221, 1.0248, 1.0293, 0.9910], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.4.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0337, 0.9992, 0.9835, 0.9741, 0.9833, 1.0056, 0.9874, 0.9845, 0.9553,\n",
      "        0.9874, 0.9974, 1.0091, 1.0213, 0.9746, 1.0064, 0.9917, 0.9801, 0.9817,\n",
      "        0.9953, 0.9689, 1.0113, 0.9649, 0.9842, 0.9747, 0.9954, 0.9994, 1.0182,\n",
      "        0.9786, 1.0031, 0.9867, 0.9683, 0.9882, 1.0476, 0.9872, 0.9698, 0.9897,\n",
      "        0.9910, 0.9767, 0.9782, 0.9705, 1.0165, 1.0155, 0.9910, 0.9909, 1.0183,\n",
      "        1.0064, 0.9957, 1.0009, 1.0403, 1.0199, 1.0117, 0.9918, 1.0023, 0.9879,\n",
      "        1.0333, 1.0244, 0.9668, 0.9690, 0.9977, 0.9689, 0.9822, 1.0118, 0.9759,\n",
      "        1.0135, 0.9922, 1.0163, 1.0060, 1.0372, 0.9973, 1.0431, 0.9863, 0.9917,\n",
      "        0.9989, 1.0307, 0.9962, 1.0103, 1.0366, 0.9716, 1.0271, 0.9831, 1.0309,\n",
      "        0.9987, 1.0100, 0.9941, 1.0051, 1.0377, 0.9850, 0.9998, 1.0162, 0.9836,\n",
      "        1.0143, 0.9874, 0.9998, 0.9848, 1.0000, 0.9960], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.5.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0320, 0.9903, 0.9922, 0.9529, 0.9777, 0.9929, 0.9932, 0.9857, 0.9816,\n",
      "        0.9977, 1.0151, 0.9964, 1.0102, 0.9775, 0.9925, 0.9959, 0.9597, 0.9771,\n",
      "        0.9872, 0.9721, 0.9977, 0.9667, 0.9871, 0.9741, 0.9849, 0.9947, 1.0110,\n",
      "        0.9596, 1.0401, 0.9930, 0.9816, 0.9965, 1.0069, 0.9656, 0.9829, 0.9893,\n",
      "        1.0048, 0.9709, 0.9759, 0.9965, 1.0001, 0.9859, 0.9609, 0.9917, 1.0127,\n",
      "        0.9856, 0.9931, 0.9913, 1.0847, 1.0255, 0.9822, 0.9926, 0.9907, 0.9791,\n",
      "        1.0318, 1.0108, 0.9978, 0.9829, 0.9901, 0.9785, 0.9736, 1.0062, 0.9579,\n",
      "        1.0169, 1.0095, 1.0079, 1.0111, 1.0147, 1.0132, 1.0230, 1.0085, 0.9818,\n",
      "        0.9758, 1.0097, 1.0042, 1.0235, 0.9728, 0.9699, 1.0506, 0.9677, 1.0276,\n",
      "        0.9897, 0.9956, 0.9920, 1.0157, 1.0304, 0.9635, 0.9788, 1.0124, 0.9763,\n",
      "        1.0101, 0.9705, 1.0141, 1.0082, 1.0017, 0.9996], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.6.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0176, 0.9916, 0.9818, 0.9617, 0.9599, 0.9902, 0.9805, 0.9810, 0.9542,\n",
      "        0.9983, 1.0029, 0.9976, 0.9794, 0.9567, 0.9781, 0.9910, 0.9786, 0.9775,\n",
      "        0.9993, 0.9868, 0.9998, 0.9824, 1.0151, 0.9669, 0.9606, 0.9730, 0.9984,\n",
      "        0.9673, 1.0215, 0.9619, 0.9495, 0.9843, 0.9966, 0.9568, 0.9822, 0.9861,\n",
      "        0.9487, 0.9595, 1.0015, 0.9954, 1.0122, 0.9907, 0.9834, 1.0063, 1.0009,\n",
      "        0.9759, 0.9965, 0.9764, 1.0312, 1.0123, 0.9912, 0.9939, 1.0019, 0.9864,\n",
      "        1.0182, 1.0153, 0.9526, 0.9739, 0.9945, 0.9962, 0.9790, 1.0029, 0.9550,\n",
      "        0.9888, 0.9897, 0.9966, 1.0041, 1.0074, 1.0135, 1.0306, 0.9762, 1.0105,\n",
      "        0.9767, 0.9966, 1.0017, 1.0125, 0.9985, 0.9578, 1.0466, 0.9767, 1.0231,\n",
      "        0.9818, 0.9874, 0.9806, 0.9908, 1.0157, 0.9504, 0.9893, 1.0048, 0.9910,\n",
      "        1.0035, 0.9699, 1.0427, 1.0039, 1.0150, 1.0072], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.7.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0063, 1.0157, 0.9905, 0.9611, 0.9776, 0.9853, 1.0028, 1.0045, 0.9744,\n",
      "        1.0008, 1.0210, 1.0097, 0.9947, 0.9789, 0.9899, 0.9993, 0.9730, 0.9812,\n",
      "        0.9924, 0.9867, 1.0003, 0.9621, 0.9984, 0.9829, 0.9708, 0.9852, 1.0144,\n",
      "        0.9481, 1.0206, 0.9781, 0.9861, 0.9782, 0.9973, 0.9718, 0.9777, 0.9903,\n",
      "        0.9868, 0.9774, 1.0096, 1.0058, 1.0198, 1.0222, 0.9712, 0.9882, 0.9993,\n",
      "        0.9974, 1.0020, 0.9843, 1.0519, 1.0055, 1.0171, 0.9880, 0.9932, 0.9988,\n",
      "        1.0476, 1.0042, 0.9765, 0.9999, 1.0130, 1.0040, 0.9706, 1.0052, 0.9624,\n",
      "        0.9978, 0.9859, 0.9871, 1.0089, 1.0379, 1.0044, 1.0129, 0.9921, 0.9990,\n",
      "        0.9739, 1.0108, 1.0004, 1.0442, 0.9844, 0.9667, 1.0374, 0.9905, 1.0175,\n",
      "        0.9557, 0.9807, 0.9963, 1.0119, 1.0289, 0.9865, 0.9927, 1.0132, 0.9883,\n",
      "        1.0005, 0.9873, 1.0172, 0.9916, 1.0076, 1.0104], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.8.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0077, 1.0233, 0.9986, 0.9647, 1.0026, 1.0134, 1.0180, 0.9915, 1.0019,\n",
      "        1.0053, 1.0226, 1.0122, 0.9798, 0.9748, 0.9991, 1.0040, 0.9897, 0.9845,\n",
      "        0.9963, 0.9675, 1.0120, 0.9801, 0.9867, 0.9719, 0.9997, 0.9738, 1.0121,\n",
      "        0.9714, 1.0153, 0.9862, 0.9878, 0.9935, 1.0101, 0.9952, 1.0051, 1.0099,\n",
      "        1.0020, 1.0055, 1.0045, 1.0136, 1.0467, 1.0370, 1.0010, 1.0052, 0.9971,\n",
      "        1.0087, 1.0157, 0.9827, 1.0338, 1.0400, 1.0039, 0.9851, 1.0155, 0.9854,\n",
      "        1.0321, 1.0170, 0.9663, 0.9819, 0.9747, 0.9976, 1.0053, 1.0183, 0.9719,\n",
      "        0.9871, 0.9853, 0.9908, 1.0253, 1.0145, 1.0392, 1.0309, 0.9988, 1.0142,\n",
      "        0.9845, 1.0149, 1.0113, 1.0217, 0.9993, 0.9766, 1.0485, 0.9730, 1.0397,\n",
      "        0.9825, 0.9948, 0.9721, 1.0086, 1.0070, 0.9872, 0.9779, 0.9881, 0.9788,\n",
      "        1.0104, 0.9916, 1.0090, 1.0256, 1.0358, 1.0108], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.9.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0167, 1.0076, 1.0051, 0.9746, 0.9907, 0.9922, 1.0249, 0.9995, 0.9502,\n",
      "        1.0130, 1.0092, 0.9980, 0.9900, 0.9699, 0.9880, 1.0004, 0.9715, 0.9812,\n",
      "        0.9927, 0.9537, 1.0156, 0.9923, 0.9868, 0.9777, 0.9787, 0.9802, 1.0003,\n",
      "        0.9478, 1.0247, 1.0114, 0.9833, 1.0048, 1.0007, 0.9767, 0.9776, 0.9926,\n",
      "        0.9885, 0.9711, 1.0159, 1.0031, 1.0250, 1.0402, 0.9679, 1.0054, 0.9839,\n",
      "        0.9858, 1.0184, 0.9694, 1.0288, 1.0328, 1.0272, 0.9609, 1.0006, 1.0033,\n",
      "        1.0203, 1.0170, 0.9910, 0.9630, 1.0082, 0.9795, 0.9779, 1.0197, 0.9468,\n",
      "        0.9899, 1.0118, 0.9922, 1.0126, 1.0211, 1.0005, 1.0117, 0.9712, 1.0334,\n",
      "        0.9892, 0.9815, 1.0391, 1.0324, 1.0041, 0.9463, 1.0410, 0.9657, 1.0063,\n",
      "        0.9669, 1.0004, 0.9979, 1.0182, 0.9838, 0.9701, 0.9946, 0.9902, 0.9782,\n",
      "        1.0096, 0.9673, 1.0276, 1.0202, 0.9995, 1.0173], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.10.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0137, 0.9975, 1.0152, 0.9310, 1.0024, 0.9871, 1.0036, 0.9833, 0.9883,\n",
      "        1.0028, 1.0033, 1.0095, 0.9989, 0.9855, 0.9918, 0.9666, 0.9860, 0.9906,\n",
      "        1.0029, 0.9848, 1.0078, 0.9921, 0.9967, 0.9627, 0.9993, 0.9923, 1.0104,\n",
      "        0.9833, 1.0218, 0.9923, 0.9716, 0.9917, 1.0113, 0.9806, 0.9908, 1.0043,\n",
      "        0.9868, 0.9917, 1.0163, 1.0003, 1.0517, 1.0303, 0.9745, 1.0056, 0.9979,\n",
      "        0.9823, 1.0385, 0.9834, 1.0116, 1.0259, 0.9822, 0.9913, 0.9985, 1.0002,\n",
      "        1.0337, 1.0055, 0.9868, 0.9713, 1.0013, 0.9750, 0.9890, 0.9816, 0.9813,\n",
      "        0.9984, 1.0039, 0.9838, 1.0219, 1.0384, 1.0104, 1.0013, 0.9571, 1.0084,\n",
      "        0.9713, 1.0048, 0.9817, 1.0200, 1.0171, 0.9526, 1.0215, 0.9648, 1.0379,\n",
      "        0.9909, 0.9972, 0.9942, 1.0055, 0.9939, 0.9906, 0.9715, 0.9829, 0.9950,\n",
      "        1.0171, 0.9780, 1.0302, 1.0261, 1.0420, 0.9955], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.11.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0225, 0.9974, 0.9994, 0.9612, 0.9940, 0.9895, 0.9977, 1.0100, 0.9933,\n",
      "        0.9890, 1.0150, 0.9869, 0.9992, 0.9900, 0.9902, 1.0188, 0.9882, 0.9934,\n",
      "        1.0113, 0.9553, 0.9957, 0.9751, 0.9753, 0.9786, 0.9984, 0.9674, 1.0221,\n",
      "        0.9890, 1.0099, 0.9840, 0.9927, 0.9826, 1.0071, 0.9662, 0.9943, 1.0182,\n",
      "        0.9952, 0.9653, 1.0098, 1.0147, 1.0193, 1.0195, 0.9752, 0.9957, 0.9724,\n",
      "        0.9936, 1.0033, 0.9758, 1.0162, 1.0277, 1.0144, 0.9739, 1.0025, 0.9844,\n",
      "        1.0309, 0.9881, 0.9760, 0.9879, 0.9779, 0.9909, 0.9788, 0.9906, 0.9820,\n",
      "        1.0065, 0.9745, 0.9804, 1.0026, 1.0407, 0.9786, 0.9827, 0.9671, 1.0045,\n",
      "        0.9696, 0.9957, 0.9954, 1.0078, 0.9868, 0.9764, 1.0044, 0.9704, 1.0440,\n",
      "        0.9495, 1.0102, 1.0004, 0.9958, 0.9753, 0.9723, 0.9958, 0.9825, 0.9838,\n",
      "        1.0067, 0.9661, 1.0506, 1.0125, 1.0287, 1.0275], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.12.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0069, 0.9992, 1.0276, 0.9838, 0.9862, 0.9881, 0.9873, 0.9799, 0.9971,\n",
      "        1.0061, 1.0155, 0.9712, 1.0033, 0.9896, 0.9965, 0.9927, 0.9789, 0.9864,\n",
      "        0.9898, 0.9669, 1.0157, 0.9685, 0.9998, 0.9891, 0.9705, 0.9798, 0.9873,\n",
      "        0.9771, 1.0070, 0.9928, 0.9819, 0.9760, 1.0038, 0.9826, 1.0106, 1.0070,\n",
      "        0.9959, 1.0002, 1.0011, 1.0127, 1.0429, 1.0114, 0.9653, 1.0045, 0.9836,\n",
      "        0.9670, 0.9961, 0.9944, 1.0007, 1.0257, 1.0213, 0.9971, 0.9923, 0.9991,\n",
      "        1.0346, 0.9916, 0.9750, 0.9778, 0.9845, 0.9823, 0.9715, 1.0061, 0.9719,\n",
      "        1.0022, 0.9957, 0.9946, 1.0196, 1.0147, 0.9977, 0.9938, 0.9873, 0.9965,\n",
      "        0.9725, 1.0060, 1.0042, 1.0293, 0.9754, 0.9763, 1.0106, 0.9921, 1.0145,\n",
      "        0.9911, 0.9908, 0.9986, 1.0012, 0.9979, 0.9805, 0.9882, 0.9727, 0.9933,\n",
      "        0.9859, 0.9804, 1.0278, 1.0408, 1.0378, 1.0160], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.13.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0129, 0.9594, 1.0135, 0.9733, 0.9712, 0.9881, 0.9935, 0.9945, 0.9834,\n",
      "        0.9964, 1.0039, 0.9819, 1.0008, 0.9843, 0.9888, 1.0150, 0.9990, 0.9882,\n",
      "        0.9928, 0.9855, 0.9903, 0.9799, 0.9848, 0.9601, 0.9823, 0.9877, 0.9985,\n",
      "        0.9796, 1.0150, 1.0011, 0.9764, 1.0038, 1.0023, 0.9818, 1.0027, 0.9961,\n",
      "        0.9890, 0.9792, 1.0194, 1.0040, 1.0279, 1.0029, 0.9813, 1.0030, 1.0006,\n",
      "        0.9825, 1.0152, 0.9887, 0.9903, 1.0159, 1.0102, 0.9602, 1.0019, 0.9872,\n",
      "        1.0006, 1.0076, 0.9736, 0.9952, 0.9829, 0.9543, 0.9758, 0.9898, 0.9570,\n",
      "        0.9856, 1.0103, 0.9818, 1.0265, 1.0209, 1.0113, 1.0079, 0.9688, 0.9992,\n",
      "        0.9801, 0.9960, 0.9893, 1.0199, 1.0015, 0.9572, 0.9976, 0.9793, 1.0129,\n",
      "        0.9713, 0.9921, 1.0029, 0.9890, 0.9976, 0.9826, 0.9956, 0.9770, 0.9741,\n",
      "        1.0313, 0.9665, 1.0254, 1.0316, 1.0333, 1.0141], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.14.0.weight\n",
      "Parameter containing:\n",
      "tensor([0.9850, 0.9718, 1.0028, 0.9667, 0.9668, 1.0087, 0.9926, 1.0091, 0.9845,\n",
      "        0.9836, 1.0120, 0.9948, 1.0077, 0.9809, 0.9904, 0.9743, 0.9745, 0.9708,\n",
      "        0.9886, 0.9932, 0.9717, 1.0152, 0.9926, 0.9928, 0.9739, 0.9783, 0.9960,\n",
      "        0.9825, 1.0104, 1.0032, 0.9902, 0.9652, 0.9921, 0.9869, 0.9868, 0.9855,\n",
      "        1.0075, 0.9794, 1.0001, 1.0110, 1.0387, 1.0239, 0.9769, 0.9983, 0.9907,\n",
      "        1.0020, 1.0010, 0.9931, 0.9910, 1.0215, 0.9913, 0.9896, 1.0094, 1.0009,\n",
      "        1.0049, 0.9774, 0.9858, 1.0077, 0.9598, 0.9655, 0.9754, 0.9985, 0.9551,\n",
      "        1.0072, 1.0032, 0.9890, 1.0345, 1.0003, 0.9915, 0.9958, 0.9676, 0.9830,\n",
      "        0.9591, 0.9989, 1.0021, 1.0245, 1.0015, 0.9644, 0.9868, 0.9789, 1.0202,\n",
      "        0.9696, 0.9805, 0.9782, 0.9953, 0.9973, 0.9755, 0.9925, 0.9663, 0.9976,\n",
      "        1.0300, 0.9616, 1.0111, 1.0056, 1.0183, 1.0227], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.15.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0187, 0.9958, 1.0149, 0.9526, 0.9615, 0.9946, 0.9841, 0.9965, 0.9794,\n",
      "        0.9873, 1.0017, 0.9742, 1.0181, 0.9838, 0.9851, 0.9800, 0.9635, 0.9775,\n",
      "        0.9952, 0.9744, 0.9957, 0.9627, 0.9819, 0.9685, 0.9932, 0.9677, 0.9948,\n",
      "        0.9884, 1.0086, 0.9735, 0.9667, 0.9857, 0.9889, 0.9834, 0.9997, 0.9998,\n",
      "        0.9890, 0.9786, 1.0026, 1.0091, 1.0320, 1.0085, 0.9802, 0.9928, 0.9905,\n",
      "        0.9979, 1.0193, 0.9879, 0.9826, 1.0027, 0.9862, 0.9754, 0.9776, 0.9859,\n",
      "        1.0188, 0.9801, 0.9736, 1.0019, 0.9830, 0.9919, 0.9844, 0.9734, 0.9627,\n",
      "        1.0022, 0.9814, 0.9741, 1.0182, 1.0079, 0.9726, 0.9796, 0.9654, 0.9750,\n",
      "        0.9589, 0.9870, 0.9883, 1.0090, 1.0039, 0.9494, 0.9748, 0.9564, 1.0063,\n",
      "        0.9880, 0.9997, 0.9883, 0.9930, 0.9580, 0.9694, 0.9849, 0.9760, 0.9958,\n",
      "        1.0162, 0.9720, 0.9866, 1.0194, 1.0280, 1.0017], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.16.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0148, 0.9812, 0.9937, 0.9695, 0.9634, 1.0041, 0.9976, 1.0073, 0.9820,\n",
      "        0.9751, 1.0022, 0.9814, 1.0100, 0.9854, 1.0078, 1.0076, 0.9814, 0.9804,\n",
      "        0.9889, 0.9799, 0.9796, 0.9744, 0.9825, 0.9927, 0.9792, 0.9786, 1.0050,\n",
      "        0.9941, 1.0278, 0.9779, 0.9886, 0.9715, 0.9802, 0.9692, 0.9874, 0.9672,\n",
      "        0.9909, 0.9671, 1.0035, 1.0181, 1.0068, 0.9923, 0.9906, 1.0062, 0.9864,\n",
      "        1.0029, 1.0114, 0.9888, 0.9883, 0.9951, 0.9946, 0.9807, 1.0128, 0.9852,\n",
      "        1.0122, 0.9824, 0.9699, 1.0013, 0.9732, 0.9744, 0.9804, 0.9801, 0.9697,\n",
      "        1.0039, 0.9775, 0.9740, 1.0363, 0.9935, 0.9910, 0.9831, 0.9533, 0.9849,\n",
      "        0.9784, 0.9670, 0.9937, 0.9988, 0.9821, 0.9706, 1.0019, 0.9905, 1.0052,\n",
      "        0.9813, 0.9884, 0.9641, 1.0002, 0.9835, 0.9857, 1.0081, 0.9922, 0.9960,\n",
      "        0.9992, 0.9745, 1.0023, 1.0024, 1.0197, 1.0148], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.17.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0076, 0.9746, 1.0074, 0.9757, 0.9890, 1.0045, 0.9752, 0.9928, 1.0062,\n",
      "        0.9940, 1.0069, 0.9980, 0.9902, 0.9782, 0.9812, 0.9827, 0.9969, 0.9602,\n",
      "        1.0084, 0.9628, 0.9874, 0.9668, 0.9781, 0.9795, 0.9734, 0.9816, 0.9899,\n",
      "        0.9963, 1.0113, 0.9946, 0.9907, 0.9849, 0.9773, 0.9692, 0.9878, 0.9830,\n",
      "        0.9639, 0.9609, 0.9919, 1.0056, 1.0367, 0.9898, 0.9850, 0.9913, 0.9969,\n",
      "        1.0104, 1.0178, 1.0039, 0.9724, 1.0066, 1.0120, 0.9739, 1.0002, 0.9892,\n",
      "        1.0108, 0.9683, 0.9644, 0.9629, 0.9705, 0.9730, 0.9797, 0.9995, 0.9689,\n",
      "        0.9961, 0.9888, 0.9922, 1.0238, 0.9834, 0.9840, 1.0064, 0.9633, 0.9880,\n",
      "        0.9793, 0.9829, 0.9875, 1.0013, 0.9823, 0.9910, 0.9959, 0.9822, 1.0047,\n",
      "        0.9681, 0.9590, 0.9641, 0.9792, 0.9944, 0.9949, 0.9991, 0.9792, 0.9823,\n",
      "        1.0227, 0.9599, 0.9894, 1.0135, 1.0294, 0.9936], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.18.0.weight\n",
      "Parameter containing:\n",
      "tensor([0.9985, 0.9812, 1.0109, 0.9541, 0.9779, 0.9976, 0.9927, 0.9883, 0.9784,\n",
      "        0.9940, 1.0021, 0.9895, 1.0018, 1.0002, 0.9813, 0.9860, 0.9692, 0.9764,\n",
      "        0.9944, 0.9690, 0.9926, 0.9976, 0.9925, 0.9853, 0.9807, 0.9727, 0.9922,\n",
      "        0.9912, 0.9963, 0.9841, 0.9772, 0.9731, 0.9552, 0.9868, 0.9689, 1.0032,\n",
      "        0.9921, 0.9891, 0.9981, 1.0053, 1.0156, 0.9921, 0.9820, 1.0078, 0.9938,\n",
      "        0.9898, 0.9983, 0.9763, 0.9717, 0.9954, 1.0018, 0.9617, 0.9988, 0.9865,\n",
      "        0.9914, 0.9580, 0.9578, 0.9870, 1.0134, 0.9895, 0.9758, 0.9957, 0.9901,\n",
      "        0.9992, 0.9793, 0.9674, 1.0286, 0.9847, 0.9965, 0.9855, 0.9837, 0.9916,\n",
      "        0.9782, 0.9851, 0.9775, 0.9976, 0.9640, 0.9812, 1.0015, 0.9875, 0.9862,\n",
      "        0.9883, 0.9765, 0.9940, 0.9873, 0.9999, 0.9655, 0.9885, 0.9719, 0.9840,\n",
      "        1.0095, 0.9715, 0.9872, 0.9815, 1.0066, 0.9962], requires_grad=True)\n",
      "(96,)\n",
      "attention_blocks.19.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0081, 0.9753, 1.0130, 0.9639, 0.9577, 0.9856, 0.9707, 1.0118, 1.0012,\n",
      "        0.9868, 0.9871, 0.9827, 0.9836, 0.9840, 0.9771, 0.9870, 0.9917, 0.9706,\n",
      "        0.9834, 0.9742, 0.9724, 0.9897, 0.9729, 0.9801, 0.9821, 0.9734, 1.0005,\n",
      "        0.9936, 1.0098, 0.9738, 0.9841, 0.9659, 0.9659, 0.9883, 0.9695, 0.9771,\n",
      "        0.9804, 0.9739, 1.0063, 0.9874, 1.0021, 0.9834, 0.9641, 0.9847, 0.9678,\n",
      "        0.9743, 1.0019, 0.9995, 0.9665, 1.0066, 0.9753, 0.9808, 0.9871, 1.0004,\n",
      "        0.9958, 0.9712, 0.9889, 0.9661, 0.9748, 0.9796, 0.9699, 0.9706, 0.9790,\n",
      "        1.0020, 0.9788, 0.9748, 1.0036, 0.9955, 0.9775, 0.9868, 0.9841, 0.9800,\n",
      "        0.9867, 0.9934, 0.9843, 0.9841, 1.0020, 0.9432, 0.9872, 0.9792, 1.0007,\n",
      "        0.9892, 0.9822, 0.9852, 0.9805, 0.9769, 0.9428, 0.9805, 0.9742, 0.9954,\n",
      "        1.0085, 0.9668, 0.9712, 1.0096, 1.0045, 0.9837], requires_grad=True)\n",
      "(96,)\n",
      "mlps.0.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0423, 1.0207, 1.0180, 1.0056, 1.0350, 0.9892, 1.0113, 1.0112, 1.0276,\n",
      "        1.0528, 1.0089, 1.0067, 1.0281, 1.0094, 1.0187, 1.0166, 1.0422, 0.9980,\n",
      "        1.0440, 0.9992, 1.0395, 1.0151, 1.0333, 0.9857, 1.0294, 1.0102, 1.0689,\n",
      "        1.0336, 1.0582, 1.0627, 1.0182, 1.0724, 1.0279, 1.0533, 1.0293, 1.0466,\n",
      "        0.9911, 1.0169, 1.0257, 1.0150, 1.0870, 1.0034, 1.0511, 1.0177, 1.0358,\n",
      "        1.0524, 1.0747, 1.0234, 1.0291, 1.0313, 1.0202, 1.0295, 1.0283, 0.9862,\n",
      "        1.0498, 1.0395, 1.0377, 0.9915, 1.0207, 1.0228, 1.0384, 1.0402, 1.0594,\n",
      "        1.0369, 1.0491, 1.0285, 1.0328, 1.0252, 1.0317, 1.0048, 1.0282, 1.0222,\n",
      "        1.0506, 1.0478, 1.0682, 1.0278, 1.0340, 1.0760, 1.0429, 1.0267, 1.0463,\n",
      "        0.9911, 1.0258, 1.0413, 1.0318, 1.0059, 1.0176, 1.0673, 1.0488, 1.0177,\n",
      "        1.0151, 1.0558, 1.0333, 1.0311, 1.0543, 1.0025], requires_grad=True)\n",
      "(96,)\n",
      "mlps.1.0.weight\n",
      "Parameter containing:\n",
      "tensor([0.9922, 1.0245, 0.9909, 1.0071, 1.0325, 1.0194, 1.0125, 1.0098, 1.0229,\n",
      "        1.0160, 1.0441, 1.0072, 1.0215, 1.0047, 1.0103, 0.9978, 1.0051, 1.0389,\n",
      "        1.0354, 1.0108, 1.0497, 1.0180, 1.0432, 1.0317, 1.0649, 1.0218, 1.0099,\n",
      "        1.0712, 1.0437, 1.0547, 1.0132, 1.0064, 1.0280, 1.0286, 1.0294, 1.0347,\n",
      "        1.0232, 1.0114, 1.0296, 1.0265, 1.0693, 1.0060, 1.0420, 1.0201, 1.0783,\n",
      "        1.0209, 1.0362, 1.0544, 1.0415, 1.0323, 1.0343, 1.0118, 1.0422, 1.0077,\n",
      "        1.0415, 1.0568, 1.0540, 1.0131, 1.0317, 1.0389, 1.0250, 1.0236, 1.0009,\n",
      "        1.0339, 1.0384, 1.0511, 1.0527, 1.0550, 1.0695, 1.0216, 1.0295, 1.0158,\n",
      "        1.0189, 1.0498, 1.0441, 1.0364, 1.0454, 1.0551, 1.0391, 1.0481, 1.0409,\n",
      "        1.0100, 1.0188, 1.0249, 1.0323, 1.0321, 1.0376, 1.0487, 1.0335, 1.0557,\n",
      "        1.0285, 1.0588, 1.0396, 1.0313, 1.0090, 1.0174], requires_grad=True)\n",
      "(96,)\n",
      "mlps.2.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0363, 1.0020, 1.0516, 1.0055, 1.0430, 1.0147, 1.0302, 1.0177, 1.0300,\n",
      "        1.0507, 1.0051, 1.0125, 1.0219, 1.0092, 1.0095, 1.0177, 1.0226, 1.0244,\n",
      "        1.0113, 1.0021, 1.0437, 1.0022, 1.0544, 1.0275, 1.0151, 1.0152, 1.0184,\n",
      "        1.0460, 1.0478, 1.0240, 1.0346, 1.0229, 1.0005, 1.0431, 0.9933, 1.0414,\n",
      "        1.0223, 1.0010, 1.0423, 1.0011, 1.0597, 1.0204, 1.0355, 1.0077, 1.0472,\n",
      "        1.0386, 1.0383, 1.0033, 1.0317, 1.0462, 1.0341, 1.0170, 0.9709, 0.9925,\n",
      "        1.0579, 1.0352, 1.0293, 1.0081, 1.0345, 0.9780, 0.9873, 1.0445, 1.0319,\n",
      "        1.0296, 1.0458, 1.0136, 1.0034, 1.0051, 1.0421, 1.0102, 1.0388, 1.0062,\n",
      "        1.0242, 1.0253, 1.0311, 1.0308, 1.0215, 1.0272, 1.0547, 1.0383, 1.0416,\n",
      "        1.0015, 1.0342, 1.0141, 1.0445, 1.0516, 1.0218, 1.0258, 1.0275, 1.0198,\n",
      "        1.0184, 1.0274, 1.0561, 1.0318, 1.0303, 1.0363], requires_grad=True)\n",
      "(96,)\n",
      "mlps.3.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0518, 1.0336, 1.0216, 1.0155, 1.0203, 1.0280, 1.0481, 0.9857, 1.0207,\n",
      "        1.0344, 1.0430, 1.0292, 1.0198, 1.0482, 1.0209, 1.0500, 1.0252, 1.0092,\n",
      "        1.0211, 1.0110, 1.0396, 1.0167, 1.0410, 1.0134, 1.0216, 1.0282, 1.0221,\n",
      "        0.9901, 1.0337, 1.0161, 1.0275, 1.0273, 1.0507, 1.0174, 1.0298, 1.0141,\n",
      "        1.0285, 1.0119, 1.0509, 1.0200, 1.0635, 1.0306, 1.0191, 1.0283, 1.0367,\n",
      "        1.0452, 1.0384, 1.0246, 1.0375, 1.0704, 1.0368, 1.0014, 1.0177, 0.9829,\n",
      "        1.0519, 1.0522, 1.0238, 1.0346, 1.0280, 1.0044, 1.0205, 1.0383, 0.9943,\n",
      "        1.0126, 1.0080, 0.9711, 1.0339, 1.0402, 1.0478, 1.0212, 1.0324, 1.0156,\n",
      "        1.0217, 1.0376, 0.9949, 1.0331, 1.0417, 1.0364, 1.0293, 1.0434, 1.0491,\n",
      "        1.0207, 1.0403, 1.0316, 0.9786, 0.9989, 1.0239, 1.0406, 1.0268, 1.0232,\n",
      "        1.0234, 1.0166, 1.0253, 1.0017, 1.0361, 1.0242], requires_grad=True)\n",
      "(96,)\n",
      "mlps.4.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0291, 1.0017, 1.0192, 0.9990, 1.0322, 1.0185, 1.0115, 1.0185, 0.9944,\n",
      "        1.0194, 1.0393, 1.0233, 0.9809, 0.9825, 1.0243, 1.0067, 1.0269, 1.0034,\n",
      "        1.0229, 1.0098, 1.0464, 1.0384, 1.0210, 0.9945, 1.0036, 1.0297, 1.0341,\n",
      "        0.9733, 1.0409, 1.0074, 1.0286, 1.0127, 1.0389, 1.0347, 1.0168, 1.0162,\n",
      "        1.0061, 1.0095, 1.0177, 1.0273, 1.0219, 1.0049, 1.0060, 1.0086, 1.0419,\n",
      "        1.0070, 1.0425, 1.0268, 1.0099, 1.0209, 1.0198, 1.0186, 1.0165, 1.0051,\n",
      "        1.0178, 1.0101, 1.0176, 0.9960, 0.9983, 1.0155, 1.0051, 1.0161, 1.0010,\n",
      "        0.9889, 0.9954, 1.0113, 1.0341, 1.0502, 1.0122, 1.0494, 1.0200, 1.0152,\n",
      "        1.0041, 1.0203, 1.0231, 1.0282, 1.0175, 1.0218, 1.0133, 1.0147, 1.0423,\n",
      "        1.0030, 1.0122, 1.0316, 1.0249, 1.0151, 0.9912, 1.0086, 1.0289, 1.0426,\n",
      "        1.0304, 1.0188, 1.0342, 1.0217, 1.0176, 1.0151], requires_grad=True)\n",
      "(96,)\n",
      "mlps.5.0.weight\n",
      "Parameter containing:\n",
      "tensor([0.9928, 1.0040, 1.0233, 0.9975, 1.0259, 1.0214, 1.0022, 1.0185, 1.0004,\n",
      "        1.0367, 1.0508, 1.0213, 1.0053, 1.0036, 1.0068, 0.9829, 1.0193, 1.0051,\n",
      "        1.0133, 0.9949, 1.0144, 1.0152, 1.0171, 1.0072, 1.0131, 1.0117, 1.0318,\n",
      "        0.9959, 1.0303, 1.0282, 1.0027, 0.9835, 1.0070, 1.0302, 1.0295, 1.0268,\n",
      "        1.0036, 1.0131, 1.0263, 1.0034, 1.0626, 1.0245, 1.0114, 1.0049, 1.0085,\n",
      "        1.0013, 1.0444, 1.0164, 1.0455, 1.0320, 1.0312, 1.0098, 1.0099, 1.0161,\n",
      "        1.0402, 1.0435, 1.0081, 1.0139, 1.0181, 1.0064, 0.9955, 1.0288, 1.0226,\n",
      "        1.0282, 1.0351, 0.9946, 1.0360, 1.0375, 1.0354, 1.0192, 1.0215, 1.0500,\n",
      "        1.0107, 1.0289, 1.0103, 1.0414, 1.0271, 1.0131, 1.0362, 1.0512, 1.0272,\n",
      "        1.0011, 0.9675, 0.9928, 1.0156, 1.0100, 1.0108, 1.0157, 1.0296, 1.0190,\n",
      "        1.0118, 1.0522, 1.0232, 1.0522, 1.0145, 1.0254], requires_grad=True)\n",
      "(96,)\n",
      "mlps.6.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0241, 1.0155, 1.0248, 1.0251, 1.0130, 1.0173, 1.0156, 1.0077, 1.0066,\n",
      "        1.0196, 1.0572, 1.0146, 1.0038, 1.0240, 1.0338, 1.0175, 1.0078, 1.0218,\n",
      "        1.0376, 0.9964, 1.0276, 1.0220, 1.0156, 0.9983, 1.0161, 1.0064, 1.0207,\n",
      "        1.0022, 1.0132, 1.0078, 1.0220, 0.9862, 1.0049, 0.9968, 1.0212, 1.0250,\n",
      "        1.0128, 1.0112, 1.0101, 1.0341, 1.0122, 1.0230, 1.0230, 1.0148, 1.0113,\n",
      "        1.0187, 1.0257, 1.0052, 1.0369, 1.0195, 1.0146, 1.0069, 1.0161, 1.0072,\n",
      "        1.0363, 1.0124, 1.0099, 1.0203, 1.0335, 1.0153, 1.0109, 1.0256, 1.0079,\n",
      "        1.0275, 1.0086, 0.9920, 1.0328, 1.0573, 1.0302, 1.0046, 1.0192, 1.0191,\n",
      "        1.0030, 0.9847, 1.0298, 1.0489, 1.0285, 1.0169, 1.0347, 1.0248, 1.0559,\n",
      "        0.9935, 1.0294, 1.0203, 1.0167, 1.0201, 1.0112, 1.0247, 1.0217, 1.0126,\n",
      "        1.0164, 1.0016, 1.0155, 1.0298, 1.0435, 1.0130], requires_grad=True)\n",
      "(96,)\n",
      "mlps.7.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0143, 1.0170, 1.0055, 1.0013, 1.0239, 1.0212, 1.0077, 0.9970, 1.0211,\n",
      "        1.0055, 1.0247, 1.0096, 0.9951, 1.0064, 1.0223, 1.0209, 1.0107, 0.9890,\n",
      "        1.0537, 1.0059, 1.0269, 1.0181, 1.0210, 1.0151, 1.0007, 0.9966, 1.0352,\n",
      "        0.9947, 1.0481, 1.0198, 1.0099, 0.9931, 1.0144, 1.0145, 0.9941, 1.0320,\n",
      "        1.0159, 1.0013, 0.9803, 1.0190, 1.0210, 1.0393, 1.0210, 1.0010, 1.0085,\n",
      "        1.0278, 1.0153, 1.0074, 1.0035, 1.0255, 1.0156, 0.9988, 1.0112, 1.0220,\n",
      "        1.0200, 0.9939, 1.0174, 0.9959, 0.9949, 1.0031, 1.0000, 1.0288, 1.0086,\n",
      "        0.9961, 0.9907, 0.9986, 1.0133, 1.0538, 1.0388, 1.0271, 1.0082, 1.0327,\n",
      "        1.0073, 1.0329, 1.0095, 1.0466, 1.0094, 1.0201, 1.0431, 1.0193, 1.0548,\n",
      "        1.0086, 1.0083, 0.9981, 1.0058, 1.0265, 0.9803, 1.0025, 1.0094, 1.0045,\n",
      "        1.0104, 0.9959, 1.0036, 1.0279, 1.0169, 1.0297], requires_grad=True)\n",
      "(96,)\n",
      "mlps.8.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0017, 1.0143, 1.0272, 1.0098, 1.0016, 1.0090, 1.0078, 1.0210, 1.0098,\n",
      "        0.9818, 1.0398, 1.0108, 0.9930, 1.0089, 1.0246, 1.0018, 1.0130, 1.0097,\n",
      "        1.0255, 1.0151, 1.0309, 1.0179, 1.0091, 1.0196, 1.0167, 0.9997, 1.0089,\n",
      "        1.0008, 1.0229, 1.0435, 1.0083, 1.0047, 0.9961, 1.0104, 1.0161, 1.0277,\n",
      "        0.9972, 1.0411, 1.0031, 1.0124, 1.0284, 1.0065, 1.0342, 1.0363, 1.0157,\n",
      "        1.0168, 1.0258, 0.9855, 1.0466, 1.0321, 1.0114, 1.0083, 1.0297, 1.0287,\n",
      "        1.0051, 0.9963, 1.0053, 1.0156, 1.0208, 1.0057, 0.9973, 1.0023, 1.0104,\n",
      "        1.0004, 0.9976, 1.0047, 1.0262, 1.0565, 1.0180, 1.0067, 1.0291, 1.0267,\n",
      "        1.0023, 1.0107, 1.0260, 1.0277, 1.0321, 1.0067, 1.0078, 1.0059, 1.0354,\n",
      "        0.9953, 0.9974, 0.9893, 0.9947, 0.9880, 1.0009, 0.9920, 1.0018, 0.9952,\n",
      "        1.0196, 0.9898, 0.9854, 1.0328, 1.0163, 1.0209], requires_grad=True)\n",
      "(96,)\n",
      "mlps.9.0.weight\n",
      "Parameter containing:\n",
      "tensor([0.9952, 1.0077, 1.0095, 1.0282, 1.0140, 1.0102, 0.9875, 1.0297, 1.0147,\n",
      "        1.0065, 1.0159, 0.9929, 0.9942, 0.9969, 1.0133, 1.0194, 0.9972, 1.0093,\n",
      "        1.0506, 1.0055, 1.0175, 1.0250, 1.0189, 0.9989, 0.9983, 1.0127, 1.0024,\n",
      "        1.0073, 1.0265, 1.0091, 1.0002, 0.9740, 1.0369, 1.0108, 1.0124, 1.0079,\n",
      "        0.9984, 0.9894, 1.0037, 1.0143, 1.0094, 1.0298, 1.0042, 1.0209, 1.0308,\n",
      "        0.9992, 1.0075, 0.9918, 1.0032, 1.0293, 1.0451, 0.9941, 1.0185, 1.0218,\n",
      "        0.9887, 1.0325, 0.9922, 1.0143, 1.0220, 0.9922, 0.9925, 1.0132, 1.0139,\n",
      "        1.0035, 1.0077, 0.9798, 1.0162, 1.0294, 1.0387, 1.0287, 0.9894, 1.0213,\n",
      "        1.0089, 1.0278, 1.0154, 1.0132, 1.0015, 1.0092, 1.0248, 1.0187, 1.0270,\n",
      "        1.0097, 1.0055, 1.0095, 0.9840, 1.0163, 1.0131, 1.0036, 1.0287, 1.0058,\n",
      "        1.0030, 0.9836, 1.0333, 1.0520, 1.0118, 1.0135], requires_grad=True)\n",
      "(96,)\n",
      "mlps.10.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0540, 1.0269, 1.0008, 1.0169, 1.0025, 0.9999, 1.0135, 1.0177, 1.0088,\n",
      "        1.0099, 1.0406, 1.0007, 0.9811, 1.0206, 1.0255, 1.0113, 1.0039, 1.0036,\n",
      "        0.9909, 1.0125, 1.0125, 1.0235, 0.9914, 0.9740, 1.0018, 1.0083, 1.0029,\n",
      "        0.9813, 1.0332, 1.0166, 1.0237, 1.0098, 1.0200, 1.0212, 1.0048, 1.0006,\n",
      "        1.0220, 1.0029, 1.0035, 1.0091, 1.0096, 1.0344, 1.0132, 1.0054, 1.0241,\n",
      "        1.0203, 1.0334, 1.0142, 1.0158, 1.0234, 1.0146, 1.0290, 1.0047, 1.0188,\n",
      "        0.9923, 1.0158, 1.0063, 1.0020, 1.0113, 0.9934, 0.9989, 1.0147, 0.9793,\n",
      "        0.9760, 1.0188, 0.9930, 1.0377, 1.0263, 1.0206, 1.0017, 0.9663, 1.0211,\n",
      "        0.9970, 1.0056, 1.0160, 1.0078, 1.0077, 1.0129, 0.9940, 1.0289, 1.0344,\n",
      "        0.9916, 0.9917, 1.0094, 1.0079, 1.0061, 1.0111, 1.0005, 0.9943, 0.9996,\n",
      "        1.0132, 0.9978, 1.0092, 1.0380, 1.0181, 0.9984], requires_grad=True)\n",
      "(96,)\n",
      "mlps.11.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0261, 0.9999, 1.0231, 1.0007, 1.0054, 1.0177, 1.0168, 1.0062, 0.9988,\n",
      "        1.0362, 1.0260, 0.9987, 1.0184, 1.0069, 1.0261, 0.9901, 0.9904, 1.0263,\n",
      "        1.0107, 0.9933, 1.0254, 1.0145, 1.0325, 0.9951, 0.9923, 0.9936, 1.0015,\n",
      "        0.9871, 0.9888, 1.0462, 1.0325, 0.9957, 1.0225, 1.0089, 1.0080, 1.0128,\n",
      "        1.0281, 1.0145, 0.9889, 0.9975, 1.0267, 0.9974, 1.0011, 0.9926, 1.0151,\n",
      "        1.0043, 1.0205, 1.0069, 1.0175, 1.0285, 0.9866, 1.0266, 1.0110, 1.0084,\n",
      "        1.0179, 1.0069, 1.0078, 1.0061, 0.9964, 1.0023, 1.0041, 1.0240, 1.0043,\n",
      "        1.0119, 1.0064, 1.0207, 1.0084, 1.0423, 1.0113, 1.0057, 1.0207, 1.0317,\n",
      "        1.0169, 1.0161, 1.0149, 1.0229, 1.0369, 1.0108, 1.0104, 1.0224, 1.0318,\n",
      "        1.0098, 1.0024, 1.0381, 1.0184, 1.0024, 1.0088, 0.9934, 1.0194, 1.0108,\n",
      "        1.0260, 0.9968, 1.0025, 1.0307, 1.0432, 0.9911], requires_grad=True)\n",
      "(96,)\n",
      "mlps.12.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0103, 1.0048, 0.9975, 0.9980, 1.0186, 0.9976, 1.0191, 1.0160, 1.0021,\n",
      "        1.0204, 1.0047, 0.9891, 1.0128, 0.9930, 1.0002, 0.9828, 1.0134, 0.9946,\n",
      "        1.0411, 0.9957, 1.0207, 1.0170, 1.0095, 0.9758, 0.9938, 0.9999, 1.0099,\n",
      "        0.9930, 1.0201, 0.9891, 1.0218, 1.0131, 1.0187, 1.0073, 1.0312, 1.0543,\n",
      "        0.9965, 0.9863, 0.9953, 1.0235, 1.0434, 1.0436, 1.0146, 1.0132, 1.0123,\n",
      "        1.0034, 1.0218, 1.0045, 1.0202, 1.0383, 1.0222, 1.0195, 1.0240, 1.0210,\n",
      "        1.0340, 0.9993, 1.0278, 1.0161, 1.0158, 1.0039, 0.9902, 1.0080, 1.0039,\n",
      "        0.9964, 0.9962, 0.9992, 1.0341, 1.0322, 1.0093, 0.9912, 1.0156, 1.0158,\n",
      "        0.9949, 0.9938, 1.0193, 1.0210, 0.9807, 1.0104, 0.9942, 0.9845, 1.0405,\n",
      "        1.0120, 1.0091, 1.0060, 1.0229, 1.0270, 1.0140, 1.0020, 0.9924, 1.0108,\n",
      "        1.0128, 1.0037, 0.9954, 0.9973, 1.0408, 1.0376], requires_grad=True)\n",
      "(96,)\n",
      "mlps.13.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0182, 1.0038, 0.9931, 1.0274, 1.0221, 1.0020, 1.0063, 1.0054, 1.0060,\n",
      "        0.9991, 1.0219, 0.9916, 1.0009, 0.9984, 0.9937, 0.9973, 1.0069, 0.9975,\n",
      "        1.0260, 1.0131, 1.0182, 0.9984, 0.9889, 0.9954, 1.0062, 1.0143, 1.0023,\n",
      "        0.9894, 1.0133, 1.0203, 1.0366, 1.0124, 1.0152, 1.0172, 0.9825, 1.0050,\n",
      "        0.9966, 1.0173, 1.0145, 0.9987, 1.0074, 0.9951, 1.0166, 1.0104, 1.0128,\n",
      "        1.0132, 1.0083, 1.0147, 0.9985, 1.0228, 0.9946, 0.9926, 1.0054, 1.0056,\n",
      "        1.0029, 1.0097, 1.0133, 1.0111, 1.0057, 1.0254, 1.0008, 1.0015, 0.9952,\n",
      "        1.0053, 1.0169, 1.0273, 1.0209, 1.0292, 1.0117, 0.9847, 1.0172, 0.9927,\n",
      "        1.0148, 0.9852, 1.0048, 1.0300, 1.0039, 1.0001, 1.0119, 1.0138, 1.0083,\n",
      "        0.9907, 1.0215, 1.0212, 1.0096, 1.0157, 0.9960, 1.0366, 0.9936, 0.9951,\n",
      "        1.0127, 0.9919, 1.0081, 1.0254, 1.0049, 1.0191], requires_grad=True)\n",
      "(96,)\n",
      "mlps.14.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0026, 0.9808, 0.9853, 0.9973, 1.0217, 1.0006, 0.9889, 1.0270, 1.0010,\n",
      "        1.0175, 1.0325, 0.9816, 1.0122, 1.0225, 0.9986, 0.9980, 0.9693, 1.0180,\n",
      "        1.0395, 1.0192, 1.0216, 1.0023, 1.0377, 1.0122, 1.0135, 1.0128, 1.0154,\n",
      "        0.9932, 1.0352, 1.0156, 0.9992, 1.0218, 0.9982, 1.0129, 1.0208, 1.0391,\n",
      "        1.0074, 1.0035, 1.0238, 1.0138, 1.0310, 0.9968, 0.9703, 0.9896, 1.0380,\n",
      "        1.0057, 1.0125, 1.0098, 1.0203, 1.0090, 0.9742, 1.0333, 1.0021, 1.0280,\n",
      "        1.0002, 1.0011, 1.0095, 1.0275, 1.0151, 1.0047, 0.9906, 1.0053, 1.0098,\n",
      "        1.0106, 0.9967, 1.0032, 0.9840, 1.0290, 1.0138, 1.0243, 1.0072, 0.9840,\n",
      "        1.0126, 0.9988, 1.0069, 1.0199, 1.0185, 0.9989, 1.0199, 1.0286, 1.0087,\n",
      "        1.0083, 1.0086, 1.0020, 1.0191, 1.0103, 1.0284, 0.9874, 1.0190, 1.0041,\n",
      "        1.0256, 1.0053, 0.9982, 1.0313, 1.0320, 1.0033], requires_grad=True)\n",
      "(96,)\n",
      "mlps.15.0.weight\n",
      "Parameter containing:\n",
      "tensor([0.9912, 1.0073, 0.9962, 0.9891, 1.0164, 1.0138, 1.0016, 1.0107, 1.0043,\n",
      "        0.9864, 1.0251, 0.9969, 1.0051, 1.0123, 0.9884, 1.0091, 0.9879, 1.0073,\n",
      "        1.0299, 0.9803, 1.0115, 0.9984, 1.0008, 0.9931, 1.0099, 1.0038, 1.0260,\n",
      "        0.9839, 1.0197, 0.9938, 1.0192, 1.0161, 0.9789, 0.9659, 0.9859, 1.0125,\n",
      "        1.0011, 1.0170, 1.0114, 1.0006, 1.0311, 1.0115, 0.9921, 1.0143, 1.0134,\n",
      "        0.9841, 1.0133, 1.0036, 1.0135, 1.0046, 1.0225, 1.0023, 0.9832, 0.9881,\n",
      "        1.0121, 0.9888, 1.0028, 1.0169, 1.0059, 1.0023, 0.9948, 1.0114, 1.0162,\n",
      "        0.9974, 1.0062, 0.9922, 0.9994, 1.0572, 0.9949, 0.9912, 0.9890, 0.9838,\n",
      "        1.0069, 1.0111, 1.0226, 1.0187, 1.0027, 0.9964, 0.9770, 1.0088, 1.0288,\n",
      "        0.9976, 1.0056, 1.0105, 1.0097, 1.0021, 1.0044, 1.0090, 1.0140, 0.9818,\n",
      "        1.0071, 1.0064, 0.9786, 1.0261, 1.0243, 1.0457], requires_grad=True)\n",
      "(96,)\n",
      "mlps.16.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0318, 0.9995, 0.9863, 1.0020, 1.0153, 1.0182, 1.0018, 1.0140, 1.0002,\n",
      "        1.0198, 0.9997, 0.9877, 1.0044, 1.0160, 0.9917, 1.0151, 0.9858, 0.9980,\n",
      "        1.0253, 0.9988, 0.9988, 1.0139, 0.9984, 1.0086, 1.0096, 1.0062, 1.0033,\n",
      "        0.9966, 1.0249, 1.0139, 1.0102, 0.9996, 1.0253, 0.9944, 0.9921, 1.0305,\n",
      "        1.0213, 1.0033, 0.9970, 1.0123, 1.0158, 1.0021, 1.0149, 1.0280, 0.9972,\n",
      "        1.0223, 1.0327, 0.9899, 1.0137, 0.9994, 1.0018, 1.0286, 1.0048, 1.0088,\n",
      "        1.0093, 0.9967, 1.0011, 1.0109, 1.0083, 0.9993, 1.0020, 1.0055, 1.0176,\n",
      "        1.0390, 0.9944, 1.0130, 0.9849, 1.0219, 1.0033, 1.0069, 1.0283, 1.0176,\n",
      "        1.0077, 0.9864, 1.0044, 1.0425, 0.9840, 1.0273, 0.9987, 1.0035, 0.9860,\n",
      "        0.9992, 0.9951, 1.0204, 1.0053, 1.0028, 1.0060, 1.0085, 1.0092, 0.9884,\n",
      "        1.0072, 1.0034, 0.9821, 1.0328, 1.0492, 1.0259], requires_grad=True)\n",
      "(96,)\n",
      "mlps.17.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0107, 1.0027, 1.0124, 1.0315, 1.0118, 0.9909, 1.0113, 1.0125, 0.9991,\n",
      "        1.0229, 1.0200, 0.9754, 1.0030, 1.0234, 0.9998, 0.9918, 1.0051, 1.0055,\n",
      "        1.0089, 1.0072, 0.9960, 1.0017, 1.0099, 1.0026, 1.0187, 1.0238, 0.9920,\n",
      "        0.9966, 1.0178, 1.0150, 1.0193, 1.0150, 1.0007, 1.0415, 1.0051, 1.0225,\n",
      "        0.9914, 0.9942, 0.9957, 1.0127, 0.9981, 1.0057, 1.0025, 1.0037, 0.9990,\n",
      "        1.0156, 1.0383, 1.0050, 1.0118, 0.9756, 0.9773, 0.9983, 0.9807, 1.0180,\n",
      "        0.9906, 1.0080, 1.0282, 0.9845, 1.0209, 0.9928, 1.0005, 0.9994, 1.0077,\n",
      "        1.0183, 0.9919, 1.0101, 1.0458, 0.9979, 1.0066, 1.0051, 1.0013, 1.0102,\n",
      "        0.9977, 1.0027, 1.0084, 1.0208, 0.9827, 0.9931, 1.0153, 1.0221, 0.9954,\n",
      "        1.0049, 1.0163, 1.0127, 1.0122, 1.0182, 1.0074, 0.9947, 1.0060, 0.9801,\n",
      "        1.0104, 0.9892, 0.9845, 1.0122, 1.0054, 1.0136], requires_grad=True)\n",
      "(96,)\n",
      "mlps.18.0.weight\n",
      "Parameter containing:\n",
      "tensor([1.0275, 0.9885, 1.0156, 1.0091, 1.0302, 1.0029, 0.9916, 1.0055, 0.9830,\n",
      "        1.0041, 1.0618, 0.9967, 0.9842, 1.0135, 1.0099, 1.0008, 1.0084, 1.0051,\n",
      "        1.0394, 1.0169, 1.0032, 1.0257, 1.0022, 1.0206, 1.0249, 1.0075, 1.0232,\n",
      "        0.9860, 1.0336, 1.0166, 1.0018, 1.0016, 1.0044, 1.0167, 1.0039, 1.0292,\n",
      "        1.0105, 1.0100, 1.0131, 1.0101, 1.0281, 0.9991, 1.0104, 0.9990, 1.0077,\n",
      "        1.0235, 1.0399, 1.0034, 1.0145, 0.9958, 0.9844, 1.0290, 1.0201, 1.0258,\n",
      "        1.0096, 0.9822, 1.0015, 1.0097, 0.9920, 0.9992, 1.0114, 1.0167, 1.0304,\n",
      "        1.0238, 0.9981, 0.9963, 0.9806, 1.0110, 0.9981, 1.0039, 1.0154, 1.0122,\n",
      "        0.9841, 1.0107, 1.0079, 1.0218, 1.0091, 1.0155, 1.0197, 1.0206, 1.0090,\n",
      "        1.0108, 1.0248, 1.0199, 1.0347, 1.0045, 1.0194, 0.9977, 1.0138, 1.0149,\n",
      "        1.0094, 1.0015, 0.9840, 1.0318, 1.0185, 1.0039], requires_grad=True)\n",
      "(96,)\n",
      "mlps.19.0.weight\n",
      "Parameter containing:\n",
      "tensor([0.9920, 0.9965, 1.0071, 0.9812, 1.0092, 1.0182, 1.0146, 0.9946, 1.0151,\n",
      "        1.0018, 1.0305, 1.0002, 0.9988, 1.0145, 0.9867, 1.0111, 1.0086, 0.9945,\n",
      "        0.9748, 1.0093, 1.0192, 1.0021, 1.0024, 0.9990, 0.9882, 0.9993, 1.0164,\n",
      "        0.9828, 1.0314, 1.0284, 0.9835, 1.0187, 0.9824, 1.0062, 0.9915, 1.0124,\n",
      "        1.0004, 0.9958, 1.0086, 1.0127, 1.0223, 0.9773, 0.9674, 1.0147, 1.0222,\n",
      "        0.9991, 1.0190, 0.9952, 1.0087, 1.0061, 0.9912, 1.0251, 0.9921, 0.9983,\n",
      "        1.0138, 1.0026, 1.0127, 1.0137, 0.9560, 0.9895, 0.9753, 1.0001, 1.0127,\n",
      "        0.9972, 0.9942, 1.0128, 1.0112, 1.0112, 1.0100, 0.9993, 1.0035, 0.9830,\n",
      "        1.0121, 0.9976, 1.0172, 0.9870, 0.9856, 1.0058, 0.9907, 0.9923, 1.0046,\n",
      "        1.0061, 0.9994, 1.0043, 1.0248, 0.9959, 1.0219, 1.0015, 0.9812, 1.0064,\n",
      "        1.0018, 0.9954, 0.9794, 0.9892, 1.0073, 1.0022], requires_grad=True)\n",
      "(96,)\n",
      "final_layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([1.0914, 1.0663, 1.0319, 1.0881, 1.0698, 1.0777, 1.0758, 1.0537, 1.0593,\n",
      "        1.0552, 1.0577, 1.0817, 1.0548, 1.0416, 1.0969, 1.0800, 1.0693, 1.0648,\n",
      "        1.0809, 1.1095, 1.0572, 1.0565, 1.0738, 1.0751, 1.0781, 1.1127, 1.0304,\n",
      "        1.0657, 1.0606, 1.0755, 1.0591, 1.0706, 1.0648, 1.0696, 1.1309, 1.0480,\n",
      "        1.0624, 1.0607, 1.0571, 1.0410, 1.0763, 1.0536, 1.0742, 1.0892, 1.0538,\n",
      "        1.0900, 1.0586, 1.0674, 1.0597, 1.0593, 1.0848, 1.0671, 1.0753, 1.0934,\n",
      "        1.0552, 1.0668, 1.0716, 1.0533, 1.0706, 1.0821, 1.0750, 1.0668, 1.0770,\n",
      "        1.0713, 1.1007, 1.0780, 1.0923, 1.0588, 1.0660, 1.1359, 1.0742, 1.0789,\n",
      "        1.0926, 1.0461, 1.0783, 1.0670, 1.0650, 1.0690, 1.0866, 1.0757, 1.0561,\n",
      "        1.0694, 1.0706, 1.1043, 1.0640, 1.0811, 1.0840, 1.0880, 1.0774, 1.0801,\n",
      "        1.0747, 1.0661, 1.0787, 1.0534, 1.0590, 1.0552], requires_grad=True)\n",
      "(96,)\n"
     ]
    }
   ],
   "source": [
    "for name, param in my_model.named_parameters():\n",
    "    moment2 = (param ** 2).mean().item()\n",
    "    if True and moment2 > 0.5:\n",
    "        print(name)\n",
    "        print(param)\n",
    "        print(tuple(param.shape))\n",
    "        \n",
    "    if False and tuple(param.shape) == (198, 96):\n",
    "        print(moment2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950fa192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, clearly the model is really bad right now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minilm",
   "language": "python",
   "name": "minilm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
