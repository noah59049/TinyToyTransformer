{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1cf4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089b062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, add_stop_token = True):\n",
    "    tokens = []\n",
    "    for letter in text:\n",
    "        if letter in TOKEN_DICT:\n",
    "            tokens.append(TOKEN_DICT[letter])\n",
    "        else:\n",
    "            # UNK token\n",
    "            tokens.append(len(TOKEN_DICT))\n",
    "            \n",
    "    # STOP token\n",
    "    if add_stop_token:\n",
    "        tokens.append(len(TOKEN_DICT) + 1)\n",
    "    return torch.tensor(tokens)\n",
    "\n",
    "def read_tokens_from_file(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        text = file.read()\n",
    "        return tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be0ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_giga_token_counts(verbose = False):\n",
    "    mega_text = \"\"\n",
    "    for directory in [\"AA\", \"AB\", \"AC\"]:\n",
    "        for digit1 in range(10):\n",
    "            for digit2 in range(10):\n",
    "                filename = f\"text/{directory}/wiki_{digit1}{digit2}\"\n",
    "                try:\n",
    "                    with open(filename, \"r\") as file:\n",
    "                        text = file.read()\n",
    "                        mega_text += text\n",
    "                except FileNotFoundError:\n",
    "                    if verbose:\n",
    "                        print(f\"File {filename} not found\")\n",
    "                    \n",
    "    return mega_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c295972",
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_text = get_giga_token_counts()\n",
    "token_counts = pd.Series(list(mega_text)).value_counts()\n",
    "token_counts = token_counts[token_counts > 300]\n",
    "TOKEN_DICT = {e:i for i, e in enumerate(token_counts.index)}\n",
    "REVERSE_TOKEN_DICT = {i:e for i, e in enumerate(token_counts.index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d1ba341",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 96\n",
    "DK = 32\n",
    "N_HEADS = 16\n",
    "CONTEXT_SIZE = 24\n",
    "MLP_HIDDEN_LAYER_SIZE = 192\n",
    "N_DISTINCT_TOKENS = len(TOKEN_DICT) + 2 # The + 2 is for UNK and STOP tokens\n",
    "N_LAYERS = 20\n",
    "P_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3258665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_matrix(dim):\n",
    "    neginf = (torch.tensor(-1) / torch.tensor(0)).item()\n",
    "    matrix = torch.zeros(dim, dim)\n",
    "    for i in range(dim):\n",
    "        for j in range(i + 1, dim):\n",
    "            matrix[i][j] = neginf\n",
    "            \n",
    "    return matrix\n",
    "MASK_MATRIX = create_mask_matrix(CONTEXT_SIZE).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133f1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embeddings(d_model, context_size):\n",
    "    a = torch.zeros(context_size, d_model)\n",
    "    for pos in range(context_size):\n",
    "        for j in range(d_model):\n",
    "            if j % 2 == 0:\n",
    "                a[pos][j] = math.sin(pos / 10000 ** (j / d_model))\n",
    "            else:\n",
    "                a[pos][j] = math.cos(pos / 10000 ** ((j - 1) / d_model))\n",
    "    return a\n",
    "                \n",
    "def positional_embeddings2(d_model, max_len):\n",
    "    # Source: https://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(\n",
    "        torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "    )\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "# These are the exact same way of getting positional embeddings, barring floating-point precision issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4462957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.Wk = nn.Linear(D_MODEL, DK, bias = False)\n",
    "        self.Wq = nn.Linear(D_MODEL, DK, bias = False)\n",
    "        self.Wv = nn.Linear(D_MODEL, DK, bias = False)\n",
    "        self.dropout = nn.Dropout(p = P_DROPOUT)\n",
    "        \n",
    "        # The weights are initialized with Kaiming uniform by default, which is fine for now\n",
    "        \n",
    "    def forward(self, x):\n",
    "        K = self.Wk(x)\n",
    "        Q = self.Wq(x)\n",
    "        V = self.Wv(x)\n",
    "        \n",
    "        QKT = torch.matmul(Q, K.transpose(dim0 = 1, dim1 = 2))\n",
    "        QKT += MASK_MATRIX\n",
    "        pattern = torch.softmax(QKT / math.sqrt(DK), dim = 2)\n",
    "        pattern = self.dropout(pattern)\n",
    "        return torch.matmul(pattern, V)\n",
    "    \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead() for i in range(N_HEADS)])\n",
    "        self.Wo = nn.Linear(DK * N_HEADS, D_MODEL, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.Wo(\n",
    "                torch.concat(\n",
    "                    [head(x) for head in self.heads],\n",
    "                        dim = 2))\n",
    "    \n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.layer1 = nn.Linear(D_MODEL, MLP_HIDDEN_LAYER_SIZE)\n",
    "        self.activation1 = nn.GELU()\n",
    "        self.layer2 = nn.Linear(MLP_HIDDEN_LAYER_SIZE, D_MODEL)\n",
    "        # The nn.Linear layers are initialized with Kaiming uniform initialization by default, which is fine\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.activation1(x1)\n",
    "        x3 = self.layer2(x2)\n",
    "        return x3\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Embedding, positional encoding, and dropout\n",
    "        self.embedding = nn.Embedding(N_DISTINCT_TOKENS, D_MODEL)\n",
    "        self.positional_embedding = positional_embeddings(D_MODEL, CONTEXT_SIZE).unsqueeze(0)\n",
    "        self.first_dropout = nn.Dropout(p = P_DROPOUT)\n",
    "\n",
    "        # Attention\n",
    "        self.attention_blocks = nn.ModuleList()\n",
    "        for i in range(N_LAYERS):\n",
    "            layer_norm = nn.LayerNorm(D_MODEL)\n",
    "            attention = AttentionBlock()\n",
    "            dropout = nn.Dropout(P_DROPOUT)\n",
    "            module = nn.Sequential(layer_norm, attention, dropout)\n",
    "            self.attention_blocks.append(module)\n",
    "            \n",
    "        # MLP\n",
    "        self.mlps = nn.ModuleList()\n",
    "        for i in range(N_LAYERS):\n",
    "            layer_norm = nn.LayerNorm(D_MODEL)\n",
    "            mlp = MLPBlock()\n",
    "            dropout = nn.Dropout(P_DROPOUT)\n",
    "            module = nn.Sequential(layer_norm, mlp, dropout)\n",
    "            self.mlps.append(module)\n",
    "            \n",
    "        # Final layer norm and linear\n",
    "        self.final_layer_norm = nn.LayerNorm(D_MODEL)\n",
    "        self.final_linear = nn.Linear(D_MODEL, N_DISTINCT_TOKENS)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Step 1: Embedding and positional encoding\n",
    "        x = self.embedding(x) + self.positional_embedding\n",
    "        \n",
    "        # Step 2: First dropout\n",
    "        x = self.first_dropout(x)\n",
    "        \n",
    "        # Step 3: All the blocks\n",
    "        for i in range(N_LAYERS):\n",
    "            x = x + self.attention_blocks[i](x)\n",
    "            x = x + self.mlps[i](x)\n",
    "            \n",
    "        # Step 4: Final layer norm\n",
    "        x = self.final_layer_norm(x)\n",
    "        \n",
    "        # Step 5: Final linear layer to get predictions\n",
    "        x = self.final_linear(x)\n",
    "        \n",
    "        # Step 6: Return\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6d592fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_peeking_test():\n",
    "    my_model = Transformer()\n",
    "    my_model.eval()\n",
    "    for pos in range(CONTEXT_SIZE):\n",
    "        x = torch.arange(0, CONTEXT_SIZE * 2, 2).unsqueeze(0)\n",
    "        output1 = my_model(x)\n",
    "        for disturb_pos in range(pos + 1, CONTEXT_SIZE):\n",
    "            x[0][disturb_pos] += 1\n",
    "            output2 = my_model(x)\n",
    "            (output1[0,:pos + 1] == output2[0,:pos + 1]).all()\n",
    "            \n",
    "no_peeking_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa7b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonzero_gradient_test():\n",
    "    my_model = Transformer()\n",
    "    x = torch.arange(0, 26, 2)\n",
    "    out = my_model(x)\n",
    "    loss = out.mean()\n",
    "    loss.backward()\n",
    "    print([e.grad for e in my_model.parameters()])\n",
    "    \n",
    "# nonzero_gradient_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5a444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonzero_gradient_test():\n",
    "    my_model = Transformer()\n",
    "\n",
    "    # 1) turn off dropout\n",
    "    my_model.eval()\n",
    "\n",
    "    # 2) (optional) fix RNG\n",
    "    torch.manual_seed(1234)\n",
    "    torch.cuda.manual_seed_all(1234)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    # 3) run forward & backward\n",
    "    x = torch.arange(0, 26, 2)           # your token indices\n",
    "    out = my_model(x)\n",
    "    loss = out.mean()\n",
    "    loss.backward()\n",
    "\n",
    "    # 4) inspect gradients\n",
    "    for name, param in my_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            g = param.grad\n",
    "            print(f\"{name:30s} → norm = {g.norm().item():.3e}\")\n",
    "        else:\n",
    "            g = param.grad\n",
    "            print(f\"DOES NOT REQUIRE GRAD: {name:30s} → norm = {g.norm().item():.3e}\")\n",
    "    \n",
    "# nonzero_gradient_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "984f8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokens):\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for i in range(0, len(tokens) - CONTEXT_SIZE - 1, CONTEXT_SIZE):\n",
    "        X = tokens[i     : i + CONTEXT_SIZE    ]\n",
    "        y = tokens[i + 1 : i + CONTEXT_SIZE + 1]\n",
    "        Xs.append(X)\n",
    "        ys.append(y)\n",
    "\n",
    "    X = torch.stack(Xs)\n",
    "    y = torch.stack(ys)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb8ff33e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loss_fn, tokens, batch_size = 32):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    X_all, y_all = get_dataset(tokens)\n",
    "    loss_vals = []\n",
    "    for i in range(0, X_all.shape[0], batch_size):\n",
    "        X = X_all[i : i + batch_size]\n",
    "        y = y_all[i : i + batch_size]\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1, N_DISTINCT_TOKENS), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             for param in model.parameters():\n",
    "#                 print(param.grad)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_vals.append(loss.item())\n",
    "        t1 = time.time()\n",
    "        \n",
    "    batch_loss = torch.mean(torch.tensor(loss_vals))\n",
    "    print(f\"{i + 1} samples evaluated, time={int(t1-t0)}, loss = {batch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88df17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch_on_file(model, optimizer, loss_fn, filename, batch_size = 32):\n",
    "    train_one_epoch(model, optimizer, loss_fn, read_tokens_from_file(filename), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb3b2e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40961 samples evaluated, time=1041, loss = 3.7226\n",
      "40961 samples evaluated, time=951, loss = 2.9642\n",
      "40961 samples evaluated, time=926, loss = 2.7970\n",
      "43009 samples evaluated, time=985, loss = 2.7214\n",
      "43009 samples evaluated, time=995, loss = 2.6637\n",
      "43009 samples evaluated, time=2594, loss = 2.5990\n",
      "43009 samples evaluated, time=1000, loss = 2.5206\n",
      "40961 samples evaluated, time=923, loss = 2.4458\n",
      "40961 samples evaluated, time=962, loss = 2.4308\n",
      "43009 samples evaluated, time=979, loss = 2.3971\n",
      "40961 samples evaluated, time=949, loss = 2.3375\n",
      "40961 samples evaluated, time=935, loss = 2.3081\n",
      "40961 samples evaluated, time=1131, loss = 2.2925\n",
      "40961 samples evaluated, time=1124, loss = 2.2495\n",
      "43009 samples evaluated, time=1129, loss = 2.2032\n",
      "43009 samples evaluated, time=1113, loss = 2.1887\n",
      "40961 samples evaluated, time=1139, loss = 2.1753\n",
      "43009 samples evaluated, time=1142, loss = 2.1543\n",
      "40961 samples evaluated, time=1114, loss = 2.1327\n",
      "43009 samples evaluated, time=998, loss = 2.0822\n",
      "40961 samples evaluated, time=996, loss = 2.0761\n",
      "40961 samples evaluated, time=1071, loss = 2.0376\n",
      "43009 samples evaluated, time=1099, loss = 2.0399\n",
      "43009 samples evaluated, time=1263, loss = 2.0315\n",
      "43009 samples evaluated, time=1072, loss = 1.9805\n",
      "40961 samples evaluated, time=1083, loss = 1.9738\n",
      "43009 samples evaluated, time=1362, loss = 1.9100\n",
      "40961 samples evaluated, time=1012, loss = 1.8450\n",
      "43009 samples evaluated, time=986, loss = 1.9380\n",
      "40961 samples evaluated, time=965, loss = 1.8931\n",
      "40961 samples evaluated, time=986, loss = 1.9251\n",
      "40961 samples evaluated, time=980, loss = 1.8794\n",
      "40961 samples evaluated, time=1076, loss = 1.8921\n",
      "43009 samples evaluated, time=1018, loss = 1.8734\n",
      "43009 samples evaluated, time=995, loss = 1.8496\n",
      "40961 samples evaluated, time=973, loss = 1.8517\n",
      "40961 samples evaluated, time=955, loss = 1.8390\n",
      "43009 samples evaluated, time=999, loss = 1.8338\n",
      "43009 samples evaluated, time=1101, loss = 1.8086\n",
      "40961 samples evaluated, time=1091, loss = 1.7860\n",
      "43009 samples evaluated, time=1011, loss = 1.8437\n",
      "40961 samples evaluated, time=966, loss = 1.7963\n",
      "40961 samples evaluated, time=973, loss = 1.7638\n",
      "43009 samples evaluated, time=1006, loss = 1.7068\n",
      "43009 samples evaluated, time=1305, loss = 1.7253\n",
      "43009 samples evaluated, time=1248, loss = 1.6939\n",
      "40961 samples evaluated, time=1130, loss = 1.6600\n",
      "40961 samples evaluated, time=1059, loss = 1.6733\n",
      "40961 samples evaluated, time=1019, loss = 1.7547\n",
      "43009 samples evaluated, time=1018, loss = 1.7207\n",
      "43009 samples evaluated, time=1001, loss = 1.7051\n",
      "40961 samples evaluated, time=963, loss = 1.7067\n",
      "43009 samples evaluated, time=1004, loss = 1.6968\n",
      "40961 samples evaluated, time=1002, loss = 1.7193\n",
      "40961 samples evaluated, time=980, loss = 1.6842\n",
      "40961 samples evaluated, time=960, loss = 1.7480\n",
      "40961 samples evaluated, time=986, loss = 1.7174\n",
      "40961 samples evaluated, time=958, loss = 1.7332\n",
      "43009 samples evaluated, time=989, loss = 1.7054\n",
      "40961 samples evaluated, time=998, loss = 1.6926\n",
      "43009 samples evaluated, time=1073, loss = 1.7037\n",
      "38913 samples evaluated, time=934, loss = 1.6817\n",
      "43009 samples evaluated, time=997, loss = 1.6764\n",
      "43009 samples evaluated, time=991, loss = 1.6619\n",
      "43009 samples evaluated, time=1006, loss = 1.6500\n",
      "40961 samples evaluated, time=1010, loss = 1.6403\n",
      "40961 samples evaluated, time=997, loss = 1.6391\n",
      "43009 samples evaluated, time=1012, loss = 1.6741\n",
      "43009 samples evaluated, time=1010, loss = 1.6888\n",
      "38913 samples evaluated, time=977, loss = 1.6970\n",
      "43009 samples evaluated, time=1028, loss = 1.6575\n",
      "40961 samples evaluated, time=1013, loss = 1.7064\n",
      "43009 samples evaluated, time=988, loss = 1.6448\n",
      "43009 samples evaluated, time=991, loss = 1.6643\n",
      "43009 samples evaluated, time=1008, loss = 1.6640\n",
      "40961 samples evaluated, time=1011, loss = 1.6806\n",
      "43009 samples evaluated, time=989, loss = 1.6610\n",
      "40961 samples evaluated, time=945, loss = 1.6813\n",
      "40961 samples evaluated, time=947, loss = 1.6809\n",
      "43009 samples evaluated, time=984, loss = 1.6508\n",
      "43009 samples evaluated, time=992, loss = 1.6578\n",
      "40961 samples evaluated, time=945, loss = 1.5824\n",
      "40961 samples evaluated, time=945, loss = 1.5906\n",
      "40961 samples evaluated, time=1052, loss = 1.5590\n",
      "38913 samples evaluated, time=936, loss = 1.6373\n",
      "43009 samples evaluated, time=1007, loss = 1.5580\n",
      "43009 samples evaluated, time=1027, loss = 1.5795\n",
      "40961 samples evaluated, time=983, loss = 1.5900\n",
      "43009 samples evaluated, time=1000, loss = 1.6143\n",
      "40961 samples evaluated, time=1109, loss = 1.6213\n",
      "40961 samples evaluated, time=996, loss = 1.6142\n",
      "43009 samples evaluated, time=1002, loss = 1.6077\n",
      "40961 samples evaluated, time=950, loss = 1.6247\n",
      "43009 samples evaluated, time=993, loss = 1.5838\n",
      "40961 samples evaluated, time=973, loss = 1.6543\n",
      "40961 samples evaluated, time=974, loss = 1.5962\n",
      "40961 samples evaluated, time=986, loss = 1.6087\n",
      "43009 samples evaluated, time=1013, loss = 1.6316\n",
      "40961 samples evaluated, time=989, loss = 1.6081\n",
      "40961 samples evaluated, time=973, loss = 1.6087\n"
     ]
    }
   ],
   "source": [
    "my_model = Transformer()\n",
    "for digit1 in range(10):\n",
    "    for digit2 in range(10):\n",
    "        run_epoch_on_file(my_model,\n",
    "                        optimizer = torch.optim.Adam(my_model.parameters()),\n",
    "                        loss_fn = nn.CrossEntropyLoss(),\n",
    "                        filename = f\"text/AA/wiki_{digit1}{digit2}\",\n",
    "                        batch_size = 2048)\n",
    "# I just realized that this resets the optimizer every epoch, which is probably a bad idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d60fb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40961 samples evaluated, time=1011, loss = 1.6003\n",
      "43009 samples evaluated, time=985, loss = 1.6147\n",
      "40961 samples evaluated, time=1070, loss = 1.6114\n",
      "43009 samples evaluated, time=1028, loss = 1.5925\n",
      "40961 samples evaluated, time=951, loss = 1.5713\n",
      "43009 samples evaluated, time=988, loss = 1.6002\n",
      "40961 samples evaluated, time=957, loss = 1.5814\n",
      "40961 samples evaluated, time=951, loss = 1.5846\n",
      "40961 samples evaluated, time=936, loss = 1.5319\n",
      "40961 samples evaluated, time=952, loss = 1.5758\n",
      "40961 samples evaluated, time=939, loss = 1.6156\n",
      "40961 samples evaluated, time=958, loss = 1.5738\n",
      "40961 samples evaluated, time=976, loss = 1.6072\n",
      "40961 samples evaluated, time=976, loss = 1.5859\n",
      "40961 samples evaluated, time=971, loss = 1.5912\n",
      "40961 samples evaluated, time=966, loss = 1.6230\n",
      "43009 samples evaluated, time=981, loss = 1.5883\n",
      "40961 samples evaluated, time=963, loss = 1.6154\n",
      "40961 samples evaluated, time=973, loss = 1.6000\n",
      "40961 samples evaluated, time=962, loss = 1.5738\n",
      "40961 samples evaluated, time=959, loss = 1.5330\n",
      "40961 samples evaluated, time=960, loss = 1.5343\n",
      "40961 samples evaluated, time=963, loss = 1.5663\n",
      "43009 samples evaluated, time=979, loss = 1.5574\n",
      "40961 samples evaluated, time=981, loss = 1.5739\n",
      "43009 samples evaluated, time=999, loss = 1.5659\n",
      "40961 samples evaluated, time=985, loss = 1.5472\n",
      "38913 samples evaluated, time=897, loss = 1.5956\n",
      "40961 samples evaluated, time=957, loss = 1.5839\n",
      "40961 samples evaluated, time=980, loss = 1.5693\n",
      "43009 samples evaluated, time=985, loss = 1.5556\n",
      "40961 samples evaluated, time=943, loss = 1.5572\n",
      "43009 samples evaluated, time=982, loss = 1.5837\n",
      "40961 samples evaluated, time=959, loss = 1.5418\n",
      "40961 samples evaluated, time=937, loss = 1.5229\n",
      "43009 samples evaluated, time=984, loss = 1.4930\n",
      "43009 samples evaluated, time=993, loss = 1.5450\n",
      "38913 samples evaluated, time=917, loss = 1.5274\n",
      "43009 samples evaluated, time=971, loss = 1.5279\n",
      "43009 samples evaluated, time=968, loss = 1.5333\n",
      "43009 samples evaluated, time=987, loss = 1.5392\n",
      "40961 samples evaluated, time=949, loss = 1.5431\n",
      "43009 samples evaluated, time=984, loss = 1.5405\n",
      "40961 samples evaluated, time=931, loss = 1.5401\n",
      "43009 samples evaluated, time=982, loss = 1.5637\n",
      "43009 samples evaluated, time=982, loss = 1.5225\n",
      "43009 samples evaluated, time=982, loss = 1.5982\n",
      "40961 samples evaluated, time=936, loss = 1.5633\n",
      "40961 samples evaluated, time=966, loss = 1.5521\n",
      "40961 samples evaluated, time=954, loss = 1.5716\n",
      "43009 samples evaluated, time=980, loss = 1.5639\n",
      "40961 samples evaluated, time=1102, loss = 1.5602\n",
      "43009 samples evaluated, time=1059, loss = 1.5676\n",
      "43009 samples evaluated, time=1015, loss = 1.5461\n",
      "43009 samples evaluated, time=1171, loss = 1.5914\n",
      "40961 samples evaluated, time=1124, loss = 1.5651\n",
      "43009 samples evaluated, time=1121, loss = 1.5844\n",
      "40961 samples evaluated, time=1062, loss = 1.5861\n",
      "43009 samples evaluated, time=1067, loss = 1.5849\n",
      "43009 samples evaluated, time=1015, loss = 1.6039\n",
      "43009 samples evaluated, time=1117, loss = 1.5978\n",
      "43009 samples evaluated, time=1133, loss = 1.5740\n",
      "40961 samples evaluated, time=1172, loss = 1.5688\n",
      "40961 samples evaluated, time=1111, loss = 1.5229\n",
      "43009 samples evaluated, time=1154, loss = 1.5546\n",
      "43009 samples evaluated, time=1177, loss = 1.4540\n",
      "40961 samples evaluated, time=1086, loss = 1.5376\n",
      "43009 samples evaluated, time=1012, loss = 1.5839\n",
      "43009 samples evaluated, time=995, loss = 1.5637\n",
      "40961 samples evaluated, time=971, loss = 1.5328\n",
      "43009 samples evaluated, time=989, loss = 1.5634\n",
      "43009 samples evaluated, time=987, loss = 1.5362\n",
      "40961 samples evaluated, time=983, loss = 1.5327\n",
      "40961 samples evaluated, time=949, loss = 1.5618\n",
      "43009 samples evaluated, time=987, loss = 1.5391\n",
      "40961 samples evaluated, time=986, loss = 1.5160\n",
      "43009 samples evaluated, time=986, loss = 1.5608\n",
      "40961 samples evaluated, time=942, loss = 1.5476\n",
      "40961 samples evaluated, time=959, loss = 1.5119\n",
      "40961 samples evaluated, time=966, loss = 1.4533\n",
      "43009 samples evaluated, time=1016, loss = 1.5296\n",
      "40961 samples evaluated, time=967, loss = 1.5236\n",
      "40961 samples evaluated, time=1004, loss = 1.5268\n",
      "43009 samples evaluated, time=1109, loss = 1.5230\n",
      "43009 samples evaluated, time=1028, loss = 1.5122\n",
      "40961 samples evaluated, time=967, loss = 1.5329\n",
      "40961 samples evaluated, time=977, loss = 1.5697\n",
      "43009 samples evaluated, time=999, loss = 1.5769\n",
      "40961 samples evaluated, time=1096, loss = 1.5783\n",
      "43009 samples evaluated, time=1110, loss = 1.5617\n",
      "40961 samples evaluated, time=1061, loss = 1.5369\n",
      "43009 samples evaluated, time=1160, loss = 1.5498\n",
      "43009 samples evaluated, time=1115, loss = 1.5586\n",
      "43009 samples evaluated, time=1004, loss = 1.5361\n",
      "43009 samples evaluated, time=983, loss = 1.5325\n",
      "43009 samples evaluated, time=1109, loss = 1.5769\n",
      "43009 samples evaluated, time=1007, loss = 1.5236\n",
      "43009 samples evaluated, time=974, loss = 1.5258\n",
      "40961 samples evaluated, time=1032, loss = 1.5156\n",
      "40961 samples evaluated, time=1138, loss = 1.4935\n"
     ]
    }
   ],
   "source": [
    "my_optimizer = torch.optim.Adam(my_model.parameters())\n",
    "for digit1 in range(10):\n",
    "    for digit2 in range(10):\n",
    "        run_epoch_on_file(my_model,\n",
    "                        optimizer = my_optimizer,\n",
    "                        loss_fn = nn.CrossEntropyLoss(),\n",
    "                        filename = f\"text/AB/wiki_{digit1}{digit2}\",\n",
    "                        batch_size = 2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c54259e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, text):\n",
    "    model.eval()\n",
    "    trimmed = text[-CONTEXT_SIZE:]\n",
    "    tokens = tokenize(trimmed, add_stop_token = False).unsqueeze(0)\n",
    "    output = model(tokens)\n",
    "    logits = output[:,0:1,:].squeeze()\n",
    "    probs = torch.softmax(logits, dim = 0) # We can do argmax just on the logits if we want, but whatever.\n",
    "    token_id = torch.argmax(logits).item()\n",
    "    return REVERSE_TOKEN_DICT[token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "658873e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_tokens(model, text, num_tokens):\n",
    "    model.eval()\n",
    "    for i in range(num_tokens):\n",
    "        text += predict_next_token(model, text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06174bc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anarchism is a political  neoen etn tntrnenhnonett  n  t h th he   e n  hhtt tthtethete ttt t tteehhthheh he h thhhththh  eeh'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_tokens(my_model, \"Anarchism is a political \", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50258feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_error(model, loss_fn, tokens, batch_size = 1024):\n",
    "    model.eval()\n",
    "    t0 = time.time()\n",
    "    X_all, y_all = get_dataset(tokens)\n",
    "    loss_vals = []\n",
    "    for i in range(0, X_all.shape[0], batch_size):\n",
    "        X = X_all[i : i + batch_size]\n",
    "        y = y_all[i : i + batch_size]\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1, N_DISTINCT_TOKENS), y.view(-1))\n",
    "        loss_vals.append(loss.item())\n",
    "        t1 = time.time()\n",
    "        \n",
    "    batch_loss = torch.mean(torch.tensor(loss_vals))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "404a5a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4638)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_test_error(my_model,\n",
    "               nn.CrossEntropyLoss(),\n",
    "               read_tokens_from_file(\"text/AD/wiki_62\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e18f33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    torch.save(model, \"models/\" + filename + \".pth\") \n",
    "    # Load this with: model = torch.load(filename)\n",
    "    # maybe do: model.eval()\n",
    "    \n",
    "    torch.save(model.state_dict(), \"models/\" + filename + \"_state_dict.pth\")\n",
    "    # Load this with:\n",
    "    # model = Transformer()\n",
    "    # model.load_state_dict(torch.load(filename))\n",
    "    # maybe do: model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb197b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(my_model, \"model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8eb348d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(my_model, \"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950fa192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, clearly the model is really bad right now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minilm",
   "language": "python",
   "name": "minilm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
